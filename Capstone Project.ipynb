{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title : The pollution problems that related to immigration in the U.S.\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Our world is facing a self-made poison that is the destruction of ecosystems, including natural resources and the environment. Waste problems, water and air pollution Including deforestation, it is undeniable that this is the poison that all humans spread to the earth. This demonstrates that no matter where humans go, there will be pollution, whether large or small. Air pollution, in particular, is one of the more and more widespread environmental problems every year, mainly due to vehicle and industrial fumes. The smoke directly affects human health.\n",
    "\n",
    "The question arises, to what extent will changes in local populations directly affect the amount of polluting gas?\n",
    "\n",
    "This project will consider only the U.S. area, which includes Immigration Data, World Temperature Data, Demographic Data, Airline data, and Pollution Data.\n",
    "All this information is collected and used to make Data Models in Data Lakes that can show data or relationships according to the questions set.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The plan for this project starts with a review of the dataset used to create a data model, which includes: Demographic Data, Immigration Data, U.S. Pollution data, World Temperature Data and Airline data. After getting the relationship between the Dataset, design data models, building the data pipelines in Data Lakes for creating the data models, check Data Quality, write SQL queries to answer questions and write summary docs.\n",
    "\n",
    "This project addresses, the environmental problem of human-caused pollution by considering the population and immigrant data that occurred in the US in each city, along with pollution data to analyze from the data how much migration occurring in the U.S. causes pollution rates.\n",
    "\n",
    "**Data Lakes** are used in this project with composed components as follows:\n",
    "1. Data Source and Target uses **AWS S3** as Object Storage.\n",
    "2. Use **Python language**, **Apache Spark** and **Data Lakes** technology to build an ETL pipeline on **AWS EMR** Cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import configparser\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col, sum, avg, max\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek, to_date\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "from workspace_utils import active_session\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Prepare config and create spark session\n",
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "    config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "    enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Describe and Gather Data \n",
    "The data sets that have been used in the project are listed below:\n",
    "\n",
    "1. **I94 Immigration Data:** This data comes from the US National Tourism and Trade Office. You can read more about it [here](https://travel.trade.gov/research/reports/i94/historical/2016.html). Data on individual travel to the U.S. for the year 2016. The data is stored in separate 'sas7bdat' files for 1 month each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "#df_spark = spark.read.format('com.github.saurfang.sas.spark').load('s3a://cnp66-bucket/Capstone_Project/DataSource/I94_Immigration Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "get_date = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(float(x))).isoformat() if x else None)\n",
    "df_Immigration = df_spark.withColumn(\"arrdate_todate\", get_date(df_spark.arrdate)).\\\n",
    "                        withColumn(\"depdate_todate\", get_date(df_spark.depdate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Mapping Immigration Data : I94ADDR_Mapping\n",
    "df_I94ADDR = spark.read.option(\"header\",True).options(delimiter=',') \\\n",
    "    .csv(\"./I94 Immigration Data-Data-2016/mapping/I94ADDR_Mapping.csv\")\n",
    "\n",
    "# Mapping Immigration Data : I94CIT_I94RES_Mapping\n",
    "df_I94CIT_I94RES = spark.read.option(\"header\",True).options(delimiter=',') \\\n",
    "    .csv(\"./I94 Immigration Data-Data-2016/mapping/I94CIT_I94RES_Mapping.csv\")\n",
    "\n",
    "# Mapping Immigration Data : I94MODE_Mapping\n",
    "df_I94MODE = spark.read.option(\"header\",True).options(delimiter=',') \\\n",
    "    .csv(\"./I94 Immigration Data-Data-2016/mapping/I94MODE_Mapping.csv\")\n",
    "\n",
    "# Mapping Immigration Data : I94PORT_Mapping\n",
    "df_I94PORT = spark.read.option(\"header\",True).options(delimiter=',') \\\n",
    "    .csv(\"./I94 Immigration Data-Data-2016/mapping/I94PORT_Mapping.csv\")\n",
    "\n",
    "# Mapping Immigration Data : I94VISA_Mapping\n",
    "df_I94VISA = spark.read.option(\"header\",True).options(delimiter=',') \\\n",
    "    .csv(\"./I94 Immigration Data-Data-2016/mapping/I94VISA_Mapping.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "2. **World Temperature Data:** This dataset came from Kaggle. You can read more about it [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data). Daily temperature data for each city and state in the U.S. is stored in a 'csv' file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#df_worldTemp = pd.read_csv(\"GlobalLandTemperaturesByCity.csv\", sep=',', engine='python',nrows=10)\n",
    "df_worldTemp = spark.read.option(\"header\",True) \\\n",
    "    .csv(\"./WorldTemperature_Data/GlobalLandTemperaturesByCity.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "3. **U.S. City Demographic Data:** This data comes from OpenSoft. You can read more about it [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/). This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. This dataset is stored as a 'csv' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#df_demog_us = pd.read_csv(\"us-cities-demographics.csv\", sep=';', engine='python',nrows=10)\n",
    "df_demog_us = spark.read.option(\"header\",True).options(delimiter=';') \\\n",
    "    .csv(\"./U.S.City_Demographic_Data/us-cities-demographics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "4. **U.S. Pollution Data:** This dataset came from Kaggle. This dataset deals with pollution in the U.S. Pollution in the U.S. has been well documented by the U.S. EPA but it is a pain to download all the data and arrange them in a format that interests data scientists. Hence I gathered four major pollutants (Nitrogen Dioxide, Sulphur Dioxide, Carbon Monoxide and Ozone) for every day from 2000 - 2016 and place them neatly in a CSV file. You can read more about it [here](https://www.kaggle.com/sogun3/uspollution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_pollution = spark.read.option(\"header\",True).options(delimiter=',') \\\n",
    "    .csv(\"./U.S.Pollution_Data/pollution_us_2000_2016.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "5. **Airline Database:** This dataset came from OpenFlights. As of January 2012, the OpenFlights Airlines Database contains 5888 airlines. Some of the information is public data and some is contributed by users. You can read more about it [here](https://openflights.org/data.html#airline). \n",
    "\n",
    "> Remark: The **Airline Database** are difference with **Airport Code Table** pre-datasets that Udacity provided to use this project beacause the Airport Code Table cannot mapping with the I94 Immigration Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#df_airlines = pd.read_csv(\"./OpenFlights/airlines.dat\", sep=',', engine='python',nrows=10)\n",
    "df_airlines = spark.read.option(\"header\",True).options(delimiter=',') \\\n",
    "    .csv(\"./OpenFlights/airlines.dat\")\n",
    "df_airlines=df_airlines.na.replace(\"\\\\N\",None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Create udf_isdate function for validating date field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.isdate(date_text)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "def isdate(date_text):\n",
    "    if date_text is not None:\n",
    "        try:\n",
    "            datetime.datetime.strptime(date_text, '%Y-%m-%d')\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "spark.udf.register('udf_isdate', isdate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Find missing values of Immigration_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arrdate_todate</th>\n",
       "      <th>chkdate</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [arrdate_todate, chkdate, cnt]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Immigration.createOrReplaceTempView(\"Immigration_data\")\n",
    "df_Immigration_missingvalue = spark.sql(\"\"\"\n",
    "                SELECT arrdate_todate, udf_isdate(arrdate_todate) as chkdate, count(*) as cnt\n",
    "                FROM Immigration_data\n",
    "                WHERE udf_isdate(arrdate_todate) = 'false'\n",
    "                GROUP BY arrdate_todate \n",
    "                ORDER BY chkdate desc\n",
    "            \"\"\")\n",
    "df_Immigration_missingvalue.limit(10).toPandas()\n",
    "#df_Immigration_missingvalue.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Find missing values of World Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>364130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AverageTemperature AverageTemperatureUncertainty     cnt\n",
       "0               None                          None  364130"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_worldTemp.createOrReplaceTempView(\"WorldTemp\")\n",
    "df_worldTemp_missingvalue1 = spark.sql(\"\"\"\n",
    "                SELECT AverageTemperature, AverageTemperatureUncertainty, count(*) as cnt \n",
    "                FROM WorldTemp\n",
    "                WHERE AverageTemperature IS NULL OR AverageTemperatureUncertainty IS NULL\n",
    "                GROUP BY AverageTemperature, AverageTemperatureUncertainty   \n",
    "                ORDER BY cnt DESC\n",
    "            \"\"\")\n",
    "df_worldTemp_missingvalue1.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Country, cnt]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_worldTemp_missingvalue2 = spark.sql(\"\"\"\n",
    "                SELECT Country, count(*) as cnt \n",
    "                FROM WorldTemp\n",
    "                WHERE Country IS NULL OR Country = ''\n",
    "                GROUP BY Country  \n",
    "                ORDER BY cnt DESC\n",
    "            \"\"\")\n",
    "df_worldTemp_missingvalue2.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Find missing values of U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [City, cnt]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demog_us.createOrReplaceTempView(\"Demographic\")\n",
    "df_demog_us_missingvalue = spark.sql(\"\"\"\n",
    "                SELECT City , count(*) as cnt \n",
    "                FROM Demographic \n",
    "                WHERE City IS NULL OR City = '' \n",
    "                GROUP BY City \n",
    "                ORDER BY City \n",
    "            \"\"\")\n",
    "df_demog_us_missingvalue.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Find missing values of U.S. Pollution Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [City, cnt]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pollution.createOrReplaceTempView(\"Pollution\")\n",
    "df_pollution_missingvalue1 = spark.sql(\"\"\"\n",
    "                SELECT City , count(*) as cnt \n",
    "                FROM Pollution \n",
    "                WHERE City IS NULL OR City = '' \n",
    "                GROUP BY City \n",
    "                ORDER BY City \n",
    "            \"\"\")\n",
    "df_pollution_missingvalue1.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_Local</th>\n",
       "      <th>chkdate</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date_Local, chkdate, cnt]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pollution_missingvalue2 = spark.sql(\"\"\"\n",
    "                SELECT `Date Local` AS Date_Local , udf_isdate(`Date Local`) as chkdate, count(*) as cnt \n",
    "                FROM Pollution \n",
    "                WHERE udf_isdate(`Date Local`) = 'false'\n",
    "                GROUP BY Date_Local \n",
    "                ORDER BY cnt DESC \n",
    "            \"\"\")\n",
    "df_pollution_missingvalue2.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Find missing values of OpenFlights (airlines) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IATA</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>4626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IATA   cnt\n",
       "0  None  4626\n",
       "1     -     2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airlines.createOrReplaceTempView(\"Airline\")\n",
    "df_airlines_missingvalue = spark.sql(\"\"\"\n",
    "                SELECT IATA, count(*) as cnt   \n",
    "                FROM Airline\n",
    "                WHERE IATA IS NULL OR IATA = '-'\n",
    "                GROUP BY IATA \n",
    "                ORDER BY cnt DESC \n",
    "            \"\"\")\n",
    "df_airlines_missingvalue.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Find missing values of TIME Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_Immigration.createOrReplaceTempView(\"Immigration_data\")\n",
    "df_worldTemp.createOrReplaceTempView(\"WorldTemp\")\n",
    "df_pollution.createOrReplaceTempView(\"Pollution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date_ref]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_temp1 = spark.sql(\"\"\"\n",
    "                SELECT DISTINCT arrdate_todate AS date_ref FROM Immigration_data WHERE arrdate_todate IS NULL\n",
    "                \"\"\")\n",
    "df_time_temp1.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  date_ref\n",
       "0     None"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_temp2 = spark.sql(\"\"\"\n",
    "                SELECT DISTINCT depdate_todate AS date_ref FROM Immigration_data WHERE depdate_todate IS NULL \n",
    "                \"\"\")\n",
    "df_time_temp2.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date_ref]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_temp3 = spark.sql(\"\"\"\n",
    "                SELECT DISTINCT dt AS date_ref FROM WorldTemp WHERE dt IS NULL \n",
    "                \"\"\")\n",
    "df_time_temp3.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date_ref]\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_temp4 = spark.sql(\"\"\"\n",
    "                SELECT DISTINCT `Date Local` AS date_ref FROM Pollution WHERE `Date Local` IS NULL \n",
    "                \"\"\")\n",
    "df_time_temp4.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_time = spark.sql(\"\"\"\n",
    "                SELECT DISTINCT arrdate_todate AS date_ref FROM Immigration_data WHERE arrdate_todate IS NOT NULL \n",
    "                UNION \n",
    "                SELECT DISTINCT depdate_todate AS date_ref FROM Immigration_data WHERE depdate_todate IS NOT NULL \n",
    "                UNION \n",
    "                SELECT DISTINCT dt AS date_ref FROM WorldTemp WHERE dt IS NOT NULL \n",
    "                UNION \n",
    "                SELECT DISTINCT `Date Local` AS date_ref FROM Pollution WHERE `Date Local` IS NOT NULL \n",
    "\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_ref</th>\n",
       "      <th>chkdate</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date_ref, chkdate, cnt]\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time.createOrReplaceTempView(\"Time\")\n",
    "df_time_final = spark.sql(\"\"\"\n",
    "            SELECT date_ref, udf_isdate(date_ref) as chkdate, count(*) AS cnt \n",
    "            FROM Time \n",
    "            WHERE udf_isdate(date_ref) = 'false'\n",
    "            GROUP BY date_ref \n",
    "            ORDER BY cnt DESC \n",
    "            \"\"\")\n",
    "df_time_final.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model:\n",
    "\n",
    "The main table of the data model is the 'Immigration' table. This data provides the individual migrant information of the U.S. and will be aggregated to find summary value by date and city for reference to other tables. The 'Demographic' table are provided information about summary people by city and state in 2015. The 'Pollution' table are provided information about air pollution (NO2, O3, SO2, and O3) by City, State, Country, and date from 2000 to 2016. The 'WorldTemp' table is provided information about World Temperature by City, Country, and date from 1743 to 2013. The 'Airline' table is provided information about the Airline including Name, IATA, ICAO, etc. And, the 'Time' table collects information about the datetime. This model demonstrates a relationship between Immigration data and Pollution data that may be relevant for one of the real factors of environmental problems. So, to come to such a conclusion requires more information and experimentation.\n",
    "\n",
    "The conceptual data model is designed as following:\n",
    "\n",
    "![DataModels](images/CapstoneProject-DataModels.drawio.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data (Data Lakes) into the chosen data model:\n",
    "\n",
    "1. Data Source is exported and placed in AWS S3\n",
    "2. Read Data files from AWS S3\n",
    "3. Run ETL process with Spark\n",
    "4. Write output data from ETL process to AWS S3\n",
    "\n",
    "Which looks like the diagram below:\n",
    "\n",
    "![Pipeline](images/CapstoneProject-Pipeline.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Create the data model of Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>i94port_city</th>\n",
       "      <th>i94port_statecode</th>\n",
       "      <th>arrdate_todate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate_todate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>biryear</th>\n",
       "      <th>gender</th>\n",
       "      <th>airline</th>\n",
       "      <th>fltno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>161.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>AUSTRIA</td>\n",
       "      <td>AUSTRIA</td>\n",
       "      <td>NEWARK/TETERBORO, NJ</td>\n",
       "      <td>NEWARK/TETERBORO</td>\n",
       "      <td>NJ</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>Air</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>2016-04-05</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>M</td>\n",
       "      <td>OS</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>AUSTRIA</td>\n",
       "      <td>AUSTRIA</td>\n",
       "      <td>FORT LAUDERDALE, FL</td>\n",
       "      <td>FORT LAUDERDALE</td>\n",
       "      <td>FL</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>Air</td>\n",
       "      <td>FLORIDA</td>\n",
       "      <td>2016-04-09</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>M</td>\n",
       "      <td>DY</td>\n",
       "      <td>07031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>345.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>AUSTRIA</td>\n",
       "      <td>AUSTRIA</td>\n",
       "      <td>LOS ANGELES, CA</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>CA</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>Air</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>F</td>\n",
       "      <td>AA</td>\n",
       "      <td>00109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>455.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>AUSTRIA</td>\n",
       "      <td>AUSTRIA</td>\n",
       "      <td>MIAMI, FL</td>\n",
       "      <td>MIAMI</td>\n",
       "      <td>FL</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>Air</td>\n",
       "      <td>FLORIDA</td>\n",
       "      <td>2016-04-17</td>\n",
       "      <td>67.0</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>00113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>766.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>AUSTRIA</td>\n",
       "      <td>AUSTRIA</td>\n",
       "      <td>CHICAGO, IL</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>IL</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>Air</td>\n",
       "      <td>ILLINOIS</td>\n",
       "      <td>2016-04-05</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>None</td>\n",
       "      <td>OS</td>\n",
       "      <td>00065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1457.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>BELGIUM</td>\n",
       "      <td>BELGIUM</td>\n",
       "      <td>NEW YORK, NY</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>NY</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>Air</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>2016-04-07</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>F</td>\n",
       "      <td>DL</td>\n",
       "      <td>00047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1535.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>BELGIUM</td>\n",
       "      <td>BELGIUM</td>\n",
       "      <td>NEW YORK, NY</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>NY</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>Air</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>2016-04-02</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>F</td>\n",
       "      <td>SN</td>\n",
       "      <td>01401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1643.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>BELGIUM</td>\n",
       "      <td>BELGIUM</td>\n",
       "      <td>MIAMI, FL</td>\n",
       "      <td>MIAMI</td>\n",
       "      <td>FL</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>Air</td>\n",
       "      <td>FLORIDA</td>\n",
       "      <td>2016-04-10</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>F</td>\n",
       "      <td>AB</td>\n",
       "      <td>07000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1654.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>BELGIUM</td>\n",
       "      <td>BELGIUM</td>\n",
       "      <td>MIAMI, FL</td>\n",
       "      <td>MIAMI</td>\n",
       "      <td>FL</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>Air</td>\n",
       "      <td>FLORIDA</td>\n",
       "      <td>2016-04-17</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>M</td>\n",
       "      <td>AA</td>\n",
       "      <td>00039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1724.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>BELGIUM</td>\n",
       "      <td>BELGIUM</td>\n",
       "      <td>MIAMI, FL</td>\n",
       "      <td>MIAMI</td>\n",
       "      <td>FL</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>Sea</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Pleasure</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>F</td>\n",
       "      <td>VES</td>\n",
       "      <td>91285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cicid   i94yr  i94mon   i94cit   i94res               i94port  \\\n",
       "0   161.0  2016.0     4.0  AUSTRIA  AUSTRIA  NEWARK/TETERBORO, NJ   \n",
       "1   166.0  2016.0     4.0  AUSTRIA  AUSTRIA   FORT LAUDERDALE, FL   \n",
       "2   345.0  2016.0     4.0  AUSTRIA  AUSTRIA       LOS ANGELES, CA   \n",
       "3   455.0  2016.0     4.0  AUSTRIA  AUSTRIA             MIAMI, FL   \n",
       "4   766.0  2016.0     4.0  AUSTRIA  AUSTRIA           CHICAGO, IL   \n",
       "5  1457.0  2016.0     4.0  BELGIUM  BELGIUM          NEW YORK, NY   \n",
       "6  1535.0  2016.0     4.0  BELGIUM  BELGIUM          NEW YORK, NY   \n",
       "7  1643.0  2016.0     4.0  BELGIUM  BELGIUM             MIAMI, FL   \n",
       "8  1654.0  2016.0     4.0  BELGIUM  BELGIUM             MIAMI, FL   \n",
       "9  1724.0  2016.0     4.0  BELGIUM  BELGIUM             MIAMI, FL   \n",
       "\n",
       "       i94port_city i94port_statecode arrdate_todate i94mode     i94addr  \\\n",
       "0  NEWARK/TETERBORO                NJ     2016-04-01     Air    NEW YORK   \n",
       "1   FORT LAUDERDALE                FL     2016-04-01     Air     FLORIDA   \n",
       "2       LOS ANGELES                CA     2016-04-01     Air  CALIFORNIA   \n",
       "3             MIAMI                FL     2016-04-01     Air     FLORIDA   \n",
       "4           CHICAGO                IL     2016-04-01     Air    ILLINOIS   \n",
       "5          NEW YORK                NY     2016-04-01     Air    NEW YORK   \n",
       "6          NEW YORK                NY     2016-04-01     Air    NEW YORK   \n",
       "7             MIAMI                FL     2016-04-01     Air     FLORIDA   \n",
       "8             MIAMI                FL     2016-04-01     Air     FLORIDA   \n",
       "9             MIAMI                FL     2016-04-01     Sea        None   \n",
       "\n",
       "  depdate_todate  i94bir   i94visa  biryear gender airline  fltno  \n",
       "0     2016-04-05    24.0  Pleasure   1992.0      M      OS     89  \n",
       "1     2016-04-09    29.0  Pleasure   1987.0      M      DY  07031  \n",
       "2     2016-04-23    35.0  Pleasure   1981.0      F      AA  00109  \n",
       "3     2016-04-17    67.0  Pleasure   1949.0   None      AA  00113  \n",
       "4     2016-04-05    31.0  Pleasure   1985.0   None      OS  00065  \n",
       "5     2016-04-07    14.0  Pleasure   2002.0      F      DL  00047  \n",
       "6     2016-04-02    45.0  Pleasure   1971.0      F      SN  01401  \n",
       "7     2016-04-10    28.0  Pleasure   1988.0      F      AB  07000  \n",
       "8     2016-04-17    65.0  Pleasure   1951.0      M      AA  00039  \n",
       "9           None    30.0  Pleasure   1986.0      F     VES  91285  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Immigration.createOrReplaceTempView(\"Immigration_data\")\n",
    "df_I94ADDR.createOrReplaceTempView(\"I94ADDR_Mapping\")\n",
    "df_I94CIT_I94RES.createOrReplaceTempView(\"I94CIT_I94RES_Mapping\")\n",
    "df_I94MODE.createOrReplaceTempView(\"I94MODE_Mapping\")\n",
    "df_I94PORT.createOrReplaceTempView(\"I94PORT_Mapping\")\n",
    "df_I94VISA.createOrReplaceTempView(\"I94VISA_Mapping\")\n",
    "df_Immigration_final = spark.sql(\"\"\"\n",
    "\n",
    "            SELECT DISTINCT cicid, i94yr, i94mon, I94CIT_I94RES1.i94cntyl as i94cit, I94CIT_I94RES2.i94cntyl as i94res, \n",
    "                trim(i94prtl) as i94port, trim(SPLIT(i94prtl,',')[0]) as i94port_city, trim(SPLIT(i94prtl,',')[1]) as i94port_statecode,\n",
    "                arrdate_todate, i94model as i94mode, i94addrl as i94addr, depdate_todate, i94bir, \n",
    "                I94VISA.I94VISA as i94visa, biryear, gender, airline, fltno   \n",
    "            FROM Immigration_data AS IMM \n",
    "            LEFT JOIN I94ADDR_Mapping AS I94ADDR on IMM.i94addr = I94ADDR.value \n",
    "            LEFT JOIN I94CIT_I94RES_Mapping AS I94CIT_I94RES1 on IMM.i94cit = I94CIT_I94RES1.value \n",
    "            LEFT JOIN I94CIT_I94RES_Mapping AS I94CIT_I94RES2 on IMM.i94res = I94CIT_I94RES2.value \n",
    "            LEFT JOIN I94MODE_Mapping AS I94MODE on IMM.i94mode = I94MODE.value \n",
    "            LEFT JOIN I94PORT_Mapping AS I94PORT on IMM.i94port = I94PORT.value \n",
    "            LEFT JOIN I94VISA_Mapping AS I94VISA on IMM.i94visa = I94VISA.value \n",
    "                \n",
    "            \"\"\")\n",
    "df_Immigration_final.limit(10).toPandas()\n",
    "#df_Immigration_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Create the data model of World Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Record_Date</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1765-11-01</td>\n",
       "      <td>3.927</td>\n",
       "      <td>4.388</td>\n",
       "      <td>ÅRHUS</td>\n",
       "      <td>DENMARK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1783-09-01</td>\n",
       "      <td>14.339</td>\n",
       "      <td>4.867</td>\n",
       "      <td>ÅRHUS</td>\n",
       "      <td>DENMARK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1824-05-01</td>\n",
       "      <td>10.503</td>\n",
       "      <td>1.9909999999999999</td>\n",
       "      <td>ÅRHUS</td>\n",
       "      <td>DENMARK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1831-12-01</td>\n",
       "      <td>2.9210000000000003</td>\n",
       "      <td>4.582</td>\n",
       "      <td>ÅRHUS</td>\n",
       "      <td>DENMARK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1864-07-01</td>\n",
       "      <td>16.343</td>\n",
       "      <td>0.7609999999999999</td>\n",
       "      <td>ÅRHUS</td>\n",
       "      <td>DENMARK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1889-11-01</td>\n",
       "      <td>4.981</td>\n",
       "      <td>0.953</td>\n",
       "      <td>ÅRHUS</td>\n",
       "      <td>DENMARK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1901-04-01</td>\n",
       "      <td>6.261</td>\n",
       "      <td>0.505</td>\n",
       "      <td>ÅRHUS</td>\n",
       "      <td>DENMARK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1901-06-01</td>\n",
       "      <td>14.654000000000002</td>\n",
       "      <td>0.516</td>\n",
       "      <td>ÅRHUS</td>\n",
       "      <td>DENMARK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1905-02-01</td>\n",
       "      <td>1.7880000000000005</td>\n",
       "      <td>0.461</td>\n",
       "      <td>ÅRHUS</td>\n",
       "      <td>DENMARK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1940-01-01</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.33299999999999996</td>\n",
       "      <td>ÅRHUS</td>\n",
       "      <td>DENMARK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Record_Date  AverageTemperature AverageTemperatureUncertainty   City  \\\n",
       "0  1765-11-01               3.927                         4.388  ÅRHUS   \n",
       "1  1783-09-01              14.339                         4.867  ÅRHUS   \n",
       "2  1824-05-01              10.503            1.9909999999999999  ÅRHUS   \n",
       "3  1831-12-01  2.9210000000000003                         4.582  ÅRHUS   \n",
       "4  1864-07-01              16.343            0.7609999999999999  ÅRHUS   \n",
       "5  1889-11-01               4.981                         0.953  ÅRHUS   \n",
       "6  1901-04-01               6.261                         0.505  ÅRHUS   \n",
       "7  1901-06-01  14.654000000000002                         0.516  ÅRHUS   \n",
       "8  1905-02-01  1.7880000000000005                         0.461  ÅRHUS   \n",
       "9  1940-01-01                -4.0           0.33299999999999996  ÅRHUS   \n",
       "\n",
       "   Country  \n",
       "0  DENMARK  \n",
       "1  DENMARK  \n",
       "2  DENMARK  \n",
       "3  DENMARK  \n",
       "4  DENMARK  \n",
       "5  DENMARK  \n",
       "6  DENMARK  \n",
       "7  DENMARK  \n",
       "8  DENMARK  \n",
       "9  DENMARK  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_worldTemp.createOrReplaceTempView(\"WorldTemp\")\n",
    "df_worldTemp_final = spark.sql(\"\"\"\n",
    "            SELECT DISTINCT dt as Record_Date, AverageTemperature, AverageTemperatureUncertainty, UPPER(City) as City, UPPER(Country) as Country  \n",
    "            FROM WorldTemp \n",
    "            WHERE AverageTemperature IS NOT NULL OR AverageTemperatureUncertainty IS NOT NULL \n",
    "            \"\"\")\n",
    "df_worldTemp_final.limit(10).toPandas()\n",
    "#df_worldTemp_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Create the data model of U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median_Age</th>\n",
       "      <th>Male_Population</th>\n",
       "      <th>Female_Population</th>\n",
       "      <th>Total_Population</th>\n",
       "      <th>NumberofVeterans</th>\n",
       "      <th>Foreign_born</th>\n",
       "      <th>Average_Household_Size</th>\n",
       "      <th>State_Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAMPTON</td>\n",
       "      <td>VIRGINIA</td>\n",
       "      <td>35.5</td>\n",
       "      <td>66214</td>\n",
       "      <td>70240</td>\n",
       "      <td>136454</td>\n",
       "      <td>19638</td>\n",
       "      <td>6204</td>\n",
       "      <td>2.48</td>\n",
       "      <td>VA</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>7513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BISMARCK</td>\n",
       "      <td>NORTH DAKOTA</td>\n",
       "      <td>38.0</td>\n",
       "      <td>34675</td>\n",
       "      <td>35565</td>\n",
       "      <td>70240</td>\n",
       "      <td>4145</td>\n",
       "      <td>2064</td>\n",
       "      <td>2.11</td>\n",
       "      <td>ND</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>4040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SAINT PAUL</td>\n",
       "      <td>MINNESOTA</td>\n",
       "      <td>31.5</td>\n",
       "      <td>149547</td>\n",
       "      <td>151293</td>\n",
       "      <td>300840</td>\n",
       "      <td>10548</td>\n",
       "      <td>56514</td>\n",
       "      <td>2.58</td>\n",
       "      <td>MN</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>27307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EL MONTE</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>37.3</td>\n",
       "      <td>57961</td>\n",
       "      <td>58784</td>\n",
       "      <td>116745</td>\n",
       "      <td>1686</td>\n",
       "      <td>59655</td>\n",
       "      <td>3.67</td>\n",
       "      <td>CA</td>\n",
       "      <td>White</td>\n",
       "      <td>64138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MURFREESBORO</td>\n",
       "      <td>TENNESSEE</td>\n",
       "      <td>30.2</td>\n",
       "      <td>60704</td>\n",
       "      <td>65417</td>\n",
       "      <td>126121</td>\n",
       "      <td>5199</td>\n",
       "      <td>8948</td>\n",
       "      <td>2.6</td>\n",
       "      <td>TN</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>8840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CARSON</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>40.4</td>\n",
       "      <td>43790</td>\n",
       "      <td>49506</td>\n",
       "      <td>93296</td>\n",
       "      <td>4273</td>\n",
       "      <td>33860</td>\n",
       "      <td>3.7</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>19947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PORT SAINT LUCIE</td>\n",
       "      <td>FLORIDA</td>\n",
       "      <td>42.1</td>\n",
       "      <td>84069</td>\n",
       "      <td>95341</td>\n",
       "      <td>179410</td>\n",
       "      <td>12416</td>\n",
       "      <td>34003</td>\n",
       "      <td>2.91</td>\n",
       "      <td>FL</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SANTA ANA</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>30.8</td>\n",
       "      <td>167503</td>\n",
       "      <td>167920</td>\n",
       "      <td>335423</td>\n",
       "      <td>4735</td>\n",
       "      <td>152999</td>\n",
       "      <td>4.58</td>\n",
       "      <td>CA</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>262436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CORONA</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>37.1</td>\n",
       "      <td>79749</td>\n",
       "      <td>84493</td>\n",
       "      <td>164242</td>\n",
       "      <td>7709</td>\n",
       "      <td>42131</td>\n",
       "      <td>2.97</td>\n",
       "      <td>CA</td>\n",
       "      <td>Asian</td>\n",
       "      <td>21044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>JERSEY CITY</td>\n",
       "      <td>NEW JERSEY</td>\n",
       "      <td>34.3</td>\n",
       "      <td>131765</td>\n",
       "      <td>132512</td>\n",
       "      <td>264277</td>\n",
       "      <td>4374</td>\n",
       "      <td>109186</td>\n",
       "      <td>2.57</td>\n",
       "      <td>NJ</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>3356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City         State Median_Age Male_Population  \\\n",
       "0           HAMPTON      VIRGINIA       35.5           66214   \n",
       "1          BISMARCK  NORTH DAKOTA       38.0           34675   \n",
       "2        SAINT PAUL     MINNESOTA       31.5          149547   \n",
       "3          EL MONTE    CALIFORNIA       37.3           57961   \n",
       "4      MURFREESBORO     TENNESSEE       30.2           60704   \n",
       "5            CARSON    CALIFORNIA       40.4           43790   \n",
       "6  PORT SAINT LUCIE       FLORIDA       42.1           84069   \n",
       "7         SANTA ANA    CALIFORNIA       30.8          167503   \n",
       "8            CORONA    CALIFORNIA       37.1           79749   \n",
       "9       JERSEY CITY    NEW JERSEY       34.3          131765   \n",
       "\n",
       "  Female_Population Total_Population NumberofVeterans Foreign_born  \\\n",
       "0             70240           136454            19638         6204   \n",
       "1             35565            70240             4145         2064   \n",
       "2            151293           300840            10548        56514   \n",
       "3             58784           116745             1686        59655   \n",
       "4             65417           126121             5199         8948   \n",
       "5             49506            93296             4273        33860   \n",
       "6             95341           179410            12416        34003   \n",
       "7            167920           335423             4735       152999   \n",
       "8             84493           164242             7709        42131   \n",
       "9            132512           264277             4374       109186   \n",
       "\n",
       "  Average_Household_Size State_Code                               Race   Count  \n",
       "0                   2.48         VA                 Hispanic or Latino    7513  \n",
       "1                   2.11         ND  American Indian and Alaska Native    4040  \n",
       "2                   2.58         MN                 Hispanic or Latino   27307  \n",
       "3                   3.67         CA                              White   64138  \n",
       "4                    2.6         TN                 Hispanic or Latino    8840  \n",
       "5                    3.7         CA          Black or African-American   19947  \n",
       "6                   2.91         FL  American Indian and Alaska Native    1536  \n",
       "7                   4.58         CA                 Hispanic or Latino  262436  \n",
       "8                   2.97         CA                              Asian   21044  \n",
       "9                   2.57         NJ  American Indian and Alaska Native    3356  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demog_us.createOrReplaceTempView(\"Demographic\")\n",
    "df_demog_us_final = spark.sql(\"\"\"\n",
    "            SELECT DISTINCT UPPER(City) as City, \n",
    "                UPPER(State) as State, \n",
    "                `Median Age` AS Median_Age, \n",
    "                `Male Population` AS Male_Population, \n",
    "                `Female Population` AS Female_Population, \n",
    "                `Total Population` AS Total_Population, \n",
    "                `Number of Veterans` AS NumberofVeterans, \n",
    "                `Foreign-born` AS Foreign_born, \n",
    "                `Average Household Size` AS Average_Household_Size, \n",
    "                `State Code` AS State_Code, \n",
    "                Race, \n",
    "                Count\n",
    "            FROM Demographic  \n",
    "                \n",
    "            \"\"\")\n",
    "df_demog_us_final.limit(10).toPandas()\n",
    "#df_demog_us_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Create the data model of U.S. Pollution Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>State_Code</th>\n",
       "      <th>County_Code</th>\n",
       "      <th>Site_Num</th>\n",
       "      <th>Address</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>City</th>\n",
       "      <th>Date_Local</th>\n",
       "      <th>NO2_Units</th>\n",
       "      <th>...</th>\n",
       "      <th>SO2_Units</th>\n",
       "      <th>SO2_Mean</th>\n",
       "      <th>SO2_1st_Max_Value</th>\n",
       "      <th>SO2_1st_Max_Hour</th>\n",
       "      <th>SO2_AQI</th>\n",
       "      <th>CO_Units</th>\n",
       "      <th>CO_Mean</th>\n",
       "      <th>CO_1st_Max_Value</th>\n",
       "      <th>CO_1st_Max_Hour</th>\n",
       "      <th>CO_AQI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>MARICOPA</td>\n",
       "      <td>PHOENIX</td>\n",
       "      <td>2000-01-26</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>MARICOPA</td>\n",
       "      <td>PHOENIX</td>\n",
       "      <td>2000-01-27</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>1.6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>349</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>MARICOPA</td>\n",
       "      <td>PHOENIX</td>\n",
       "      <td>2000-03-28</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>0.226087</td>\n",
       "      <td>0.8</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>705</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>MARICOPA</td>\n",
       "      <td>PHOENIX</td>\n",
       "      <td>2000-07-03</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.495833</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>849</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>MARICOPA</td>\n",
       "      <td>PHOENIX</td>\n",
       "      <td>2000-08-10</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>3.434783</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>917</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>MARICOPA</td>\n",
       "      <td>PHOENIX</td>\n",
       "      <td>2000-08-27</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2969</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>1011</td>\n",
       "      <td>1237 S. BEVERLY , TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>PIMA</td>\n",
       "      <td>TUCSON</td>\n",
       "      <td>2000-08-14</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5741</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>1002</td>\n",
       "      <td>5551 BETHEL ISLAND RD</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>CONTRA COSTA</td>\n",
       "      <td>BETHEL ISLAND</td>\n",
       "      <td>2000-07-24</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>3.136364</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8061</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>3001</td>\n",
       "      <td>583 W. 10TH ST., PITTSBURG</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>CONTRA COSTA</td>\n",
       "      <td>PITTSBURG</td>\n",
       "      <td>2000-02-23</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8485</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>3001</td>\n",
       "      <td>583 W. 10TH ST., PITTSBURG</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>CONTRA COSTA</td>\n",
       "      <td>PITTSBURG</td>\n",
       "      <td>2000-06-10</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     No State_Code County_Code Site_Num  \\\n",
       "0   101          4          13     3002   \n",
       "1   105          4          13     3002   \n",
       "2   349          4          13     3002   \n",
       "3   705          4          13     3002   \n",
       "4   849          4          13     3002   \n",
       "5   917          4          13     3002   \n",
       "6  2969          4          19     1011   \n",
       "7  5741          6          13     1002   \n",
       "8  8061          6          13     3001   \n",
       "9  8485          6          13     3001   \n",
       "\n",
       "                                   Address       State        County  \\\n",
       "0  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN     ARIZONA      MARICOPA   \n",
       "1  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN     ARIZONA      MARICOPA   \n",
       "2  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN     ARIZONA      MARICOPA   \n",
       "3  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN     ARIZONA      MARICOPA   \n",
       "4  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN     ARIZONA      MARICOPA   \n",
       "5  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN     ARIZONA      MARICOPA   \n",
       "6                 1237 S. BEVERLY , TUCSON     ARIZONA          PIMA   \n",
       "7                    5551 BETHEL ISLAND RD  CALIFORNIA  CONTRA COSTA   \n",
       "8               583 W. 10TH ST., PITTSBURG  CALIFORNIA  CONTRA COSTA   \n",
       "9               583 W. 10TH ST., PITTSBURG  CALIFORNIA  CONTRA COSTA   \n",
       "\n",
       "            City  Date_Local          NO2_Units  ...            SO2_Units  \\\n",
       "0        PHOENIX  2000-01-26  Parts per billion  ...    Parts per billion   \n",
       "1        PHOENIX  2000-01-27  Parts per billion  ...    Parts per billion   \n",
       "2        PHOENIX  2000-03-28  Parts per billion  ...    Parts per billion   \n",
       "3        PHOENIX  2000-07-03  Parts per billion  ...    Parts per billion   \n",
       "4        PHOENIX  2000-08-10  Parts per billion  ...    Parts per billion   \n",
       "5        PHOENIX  2000-08-27  Parts per billion  ...    Parts per billion   \n",
       "6         TUCSON  2000-08-14  Parts per billion  ...    Parts per billion   \n",
       "7  BETHEL ISLAND  2000-07-24  Parts per billion  ...    Parts per billion   \n",
       "8      PITTSBURG  2000-02-23  Parts per billion  ...    Parts per billion   \n",
       "9      PITTSBURG  2000-06-10  Parts per billion  ...    Parts per billion   \n",
       "\n",
       "   SO2_Mean  SO2_1st_Max_Value  SO2_1st_Max_Hour SO2_AQI           CO_Units  \\\n",
       "0  3.363636                7.0               1.0    10.0  Parts per million   \n",
       "1  3.684211               11.0               8.0    16.0  Parts per million   \n",
       "2  0.226087                0.8              21.0     0.0  Parts per million   \n",
       "3  0.272727                1.0               4.0     1.0  Parts per million   \n",
       "4  3.434783                9.0              22.0    13.0  Parts per million   \n",
       "5  0.250000                2.0               1.0     3.0  Parts per million   \n",
       "6  2.000000                2.0               0.0     3.0  Parts per million   \n",
       "7  3.136364                5.0              11.0     7.0  Parts per million   \n",
       "8  1.428571                9.0               9.0    13.0  Parts per million   \n",
       "9  0.681818                2.0              11.0     3.0  Parts per million   \n",
       "\n",
       "    CO_Mean  CO_1st_Max_Value  CO_1st_Max_Hour CO_AQI  \n",
       "0  0.866667               1.4              0.0   16.0  \n",
       "1  0.925000               1.6             23.0   18.0  \n",
       "2  0.508333               0.7             12.0    8.0  \n",
       "3  0.495833               0.7              4.0    8.0  \n",
       "4  0.312500               0.6             23.0    7.0  \n",
       "5  0.408333               0.5              0.0    6.0  \n",
       "6  0.266667               0.3              0.0    3.0  \n",
       "7  0.333333               0.4              1.0    5.0  \n",
       "8  0.800000               0.8             19.0    9.0  \n",
       "9  0.300000               0.3              0.0    3.0  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pollution.createOrReplaceTempView(\"Pollution\")\n",
    "df_pollution_final = spark.sql(\"\"\"\n",
    "            SELECT DISTINCT _c0 AS No,\n",
    "                `State Code` AS State_Code,\n",
    "                `County Code` AS County_Code,\n",
    "                `Site Num` AS Site_Num,\n",
    "                `Address` AS Address,\n",
    "                UPPER(State) AS State,\n",
    "                UPPER(County) AS County,\n",
    "                UPPER(City) AS City,\n",
    "                `Date Local` AS Date_Local,\n",
    "                `NO2 Units` AS NO2_Units,\n",
    "                DOUBLE(`NO2 Mean`) AS NO2_Mean,\n",
    "                DOUBLE(`NO2 1st Max Value`) AS NO2_1st_Max_Value,\n",
    "                DOUBLE(`NO2 1st Max Hour`) AS NO2_1st_Max_Hour,\n",
    "                DOUBLE(`NO2 AQI`) AS NO2_AQI,\n",
    "                `O3 Units` AS O3_Units,\n",
    "                DOUBLE(`O3 Mean`) AS O3_Mean,\n",
    "                DOUBLE(`O3 1st Max Value`) AS O3_1st_Max_Value,\n",
    "                DOUBLE(`O3 1st Max Hour`) AS O3_1st_Max_Hour,\n",
    "                DOUBLE(`O3 AQI`) AS O3_AQI,\n",
    "                `SO2 Units` AS SO2_Units,\n",
    "                DOUBLE(`SO2 Mean`) AS SO2_Mean,\n",
    "                DOUBLE(`SO2 1st Max Value`) AS SO2_1st_Max_Value,\n",
    "                DOUBLE(`SO2 1st Max Hour`) AS SO2_1st_Max_Hour,\n",
    "                DOUBLE(`SO2 AQI`) AS SO2_AQI,\n",
    "                `CO Units` AS CO_Units,\n",
    "                DOUBLE(`CO Mean`) AS CO_Mean,\n",
    "                DOUBLE(`CO 1st Max Value`) AS CO_1st_Max_Value,\n",
    "                DOUBLE(`CO 1st Max Hour`) AS CO_1st_Max_Hour,\n",
    "                DOUBLE(`CO AQI`) AS CO_AQI\n",
    "            FROM Pollution  \n",
    "                \n",
    "            \"\"\")\n",
    "\n",
    "df_pollution_final = df_pollution_final.na.drop()\n",
    "df_pollution_final.limit(10).toPandas()\n",
    "#df_pollution_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Create the data model of OpenFlights (airlines) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airline_ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Alias</th>\n",
       "      <th>IATA</th>\n",
       "      <th>ICAO</th>\n",
       "      <th>Callsign</th>\n",
       "      <th>Country</th>\n",
       "      <th>Active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1516</td>\n",
       "      <td>BAX Global</td>\n",
       "      <td>None</td>\n",
       "      <td>8W</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3186</td>\n",
       "      <td>Kyrgyzstan Airlines</td>\n",
       "      <td>None</td>\n",
       "      <td>R8</td>\n",
       "      <td>KGA</td>\n",
       "      <td>KYRGYZ</td>\n",
       "      <td>KYRGYZSTAN</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3274</td>\n",
       "      <td>Linea Turistica Aerotuy</td>\n",
       "      <td>None</td>\n",
       "      <td>LD</td>\n",
       "      <td>TUY</td>\n",
       "      <td>AEREOTUY</td>\n",
       "      <td>VENEZUELA</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3287</td>\n",
       "      <td>Linhas A</td>\n",
       "      <td>None</td>\n",
       "      <td>LM</td>\n",
       "      <td>LAM</td>\n",
       "      <td>MOZAMBIQUE</td>\n",
       "      <td>MOZAMBIQUE</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3835</td>\n",
       "      <td>PB Air</td>\n",
       "      <td>None</td>\n",
       "      <td>9Q</td>\n",
       "      <td>PBA</td>\n",
       "      <td>PEEBEE AIR</td>\n",
       "      <td>THAILAND</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4356</td>\n",
       "      <td>Sun Country Airlines</td>\n",
       "      <td>None</td>\n",
       "      <td>SY</td>\n",
       "      <td>SCX</td>\n",
       "      <td>SUN COUNTRY</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9082</td>\n",
       "      <td>Uni Air</td>\n",
       "      <td>None</td>\n",
       "      <td>B7</td>\n",
       "      <td>UIA</td>\n",
       "      <td>Glory</td>\n",
       "      <td>TAIWAN</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11948</td>\n",
       "      <td>Viking Hellas</td>\n",
       "      <td>None</td>\n",
       "      <td>VQ</td>\n",
       "      <td>VKH</td>\n",
       "      <td>DELPHI</td>\n",
       "      <td>GREECE</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15930</td>\n",
       "      <td>Airlink (SAA)</td>\n",
       "      <td>None</td>\n",
       "      <td>4Z</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SOUTH AFRICA</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15970</td>\n",
       "      <td>Zuliana de Aviacion</td>\n",
       "      <td>Zuliana</td>\n",
       "      <td>OD</td>\n",
       "      <td>ULA</td>\n",
       "      <td>None</td>\n",
       "      <td>VENEZUELA</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Airline_ID                     Name    Alias IATA  ICAO     Callsign  \\\n",
       "0       1516               BAX Global     None   8W  None         None   \n",
       "1       3186      Kyrgyzstan Airlines     None   R8   KGA       KYRGYZ   \n",
       "2       3274  Linea Turistica Aerotuy     None   LD   TUY     AEREOTUY   \n",
       "3       3287                 Linhas A     None   LM   LAM   MOZAMBIQUE   \n",
       "4       3835                   PB Air     None   9Q   PBA   PEEBEE AIR   \n",
       "5       4356     Sun Country Airlines     None   SY   SCX  SUN COUNTRY   \n",
       "6       9082                  Uni Air     None   B7   UIA        Glory   \n",
       "7      11948            Viking Hellas     None   VQ   VKH       DELPHI   \n",
       "8      15930            Airlink (SAA)     None   4Z  None         None   \n",
       "9      15970      Zuliana de Aviacion  Zuliana   OD   ULA         None   \n",
       "\n",
       "         Country Active  \n",
       "0           None      N  \n",
       "1     KYRGYZSTAN      N  \n",
       "2      VENEZUELA      N  \n",
       "3     MOZAMBIQUE      Y  \n",
       "4       THAILAND      Y  \n",
       "5  UNITED STATES      Y  \n",
       "6         TAIWAN      Y  \n",
       "7         GREECE      Y  \n",
       "8   SOUTH AFRICA      Y  \n",
       "9      VENEZUELA      N  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airlines.createOrReplaceTempView(\"Airline\")\n",
    "df_airlines_final = spark.sql(\"\"\"\n",
    "            SELECT DISTINCT AirlineID AS Airline_ID, \n",
    "                Name, \n",
    "                Alias, \n",
    "                IATA,\n",
    "                ICAO,\n",
    "                Callsign,\n",
    "                UPPER(Country) AS Country,\n",
    "                Active  \n",
    "            FROM Airline\n",
    "            WHERE IATA IS NOT NULL OR IATA <> '-'\n",
    "\n",
    "            \"\"\")\n",
    "df_airlines_final.limit(10).toPandas()\n",
    "#df_airlines_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Create the data model of TIME Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1820-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1839-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1853-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1892-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1893-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1924-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1955-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1992-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1994-03-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_ref\n",
       "0  2016-08-17\n",
       "1  1820-12-01\n",
       "2  1839-05-01\n",
       "3  1853-08-01\n",
       "4  1892-02-01\n",
       "5  1893-09-01\n",
       "6  1924-07-01\n",
       "7  1955-03-01\n",
       "8  1992-05-01\n",
       "9  1994-03-01"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Immigration.createOrReplaceTempView(\"Immigration_data\")\n",
    "df_worldTemp.createOrReplaceTempView(\"WorldTemp\")\n",
    "df_pollution.createOrReplaceTempView(\"Pollution\")\n",
    "\n",
    "df_time_final = spark.sql(\"\"\"\n",
    "                SELECT DISTINCT arrdate_todate AS date_ref FROM Immigration_data WHERE arrdate_todate IS NOT NULL \n",
    "                UNION \n",
    "                SELECT DISTINCT depdate_todate AS date_ref FROM Immigration_data WHERE depdate_todate IS NOT NULL \n",
    "                UNION \n",
    "                SELECT DISTINCT dt AS date_ref FROM WorldTemp WHERE dt IS NOT NULL \n",
    "                UNION \n",
    "                SELECT DISTINCT `Date Local` AS date_ref FROM Pollution WHERE `Date Local` IS NOT NULL \n",
    "            \"\"\")\n",
    "df_time_final.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Extract columns to create time table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_ref</th>\n",
       "      <th>day</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-17</td>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1820-12-01</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "      <td>1820</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1839-05-01</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>1839</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1853-08-01</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>1853</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1892-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1892</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1893-09-01</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>1893</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1924-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>1924</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1955-03-01</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1955</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1992-05-01</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1994-03-01</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_ref  day  week  month  year  weekday\n",
       "0  2016-08-17   17    33      8  2016        4\n",
       "1  1820-12-01    1    48     12  1820        6\n",
       "2  1839-05-01    1    18      5  1839        4\n",
       "3  1853-08-01    1    31      8  1853        2\n",
       "4  1892-02-01    1     5      2  1892        2\n",
       "5  1893-09-01    1    35      9  1893        6\n",
       "6  1924-07-01    1    27      7  1924        3\n",
       "7  1955-03-01    1     9      3  1955        3\n",
       "8  1992-05-01    1    18      5  1992        6\n",
       "9  1994-03-01    1     9      3  1994        3"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_final = df_time_final.withColumn('day',dayofmonth(df_time_final.date_ref)) \\\n",
    "    .withColumn('week',weekofyear(df_time_final.date_ref)) \\\n",
    "    .withColumn('month',month(df_time_final.date_ref)) \\\n",
    "    .withColumn('year',year(df_time_final.date_ref)) \\\n",
    "    .withColumn('weekday',dayofweek(df_time_final.date_ref)) \n",
    "df_time_final.limit(10).toPandas()\n",
    "#df_time_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 How to run the Python Scripts\n",
    "\n",
    "**Run on AWS EMR**\n",
    "\n",
    "1. Create AWS EMR Cluster as following link: \n",
    "    - [Udacity: AWS CLI - Create EMR Cluster](https://classroom.udacity.com/nanodegrees/nd027/parts/67bd4916-3fc3-4474-b0fd-197dc014e709/modules/3671171a-8bf9-4f75-a913-37439ad281b3/lessons/f08d8ac9-b44a-4717-9da1-9714829da6c9/concepts/9c346cbc-5cf9-4e76-bdf5-a81ce94b55fe)\n",
    "    - [Udacity: How to execute ETL of Data Lake project on Spark Cluster in AWS?](https://knowledge.udacity.com/questions/46619#552992)\n",
    "\n",
    "    Or, create AWS EMR Cluster with AWS CLI as follow example below:\n",
    "\n",
    "    ``` shell\n",
    "    aws emr create-cluster --name <INPUT_NAME> --use-default-roles --release-label emr-5.34.0 --instance-count 3 --applications Name=Spark Name=JupyterEnterpriseGateway --ec2-attributes KeyName=<INPUT_KEYNAME>,SubnetId=<INPUT_SUBNET> --instance-type m5.xlarge --region us-west-2 --profile <INPUT_PROFILE>\n",
    "    \n",
    "    # Example:\n",
    "    aws emr create-cluster --name emr-cluster-capstone-project-13 --use-default-roles --release-label emr-5.34.0 --instance-count 3 --applications Name=Spark Name=JupyterEnterpriseGateway --ec2-attributes KeyName=emr-cluster-demo_4,SubnetId=subnet-0f50ed7eb6774646a --instance-type m5.xlarge --region us-west-2 --profile cnpawsadmin\n",
    "    \n",
    "    # And, you will receive result messages as below:\n",
    "    \n",
    "    {\n",
    "    \"ClusterId\": \"j-2GMHZ63X7KGYA\",\n",
    "    \"ClusterArn\": \"arn:aws:elasticmapreduce:us-west-2:453256782086:cluster/j-2GMHZ63X7KGYA\"\n",
    "    }\n",
    "    \n",
    "    ```\n",
    "    \n",
    "\n",
    "2. Connect the AWS EMR Cluster with SSH as following link : [Connect to the master node using SSH](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-connect-master-node-ssh.html)\n",
    "\n",
    "3. Run command to configure the AWS CLI on EMR cluster.\n",
    "\n",
    "    ``` bash\n",
    "    export AWS_ACCESS_KEY_ID=<input_your_key_id>\n",
    "    export AWS_SECRET_ACCESS_KEY=<input_your_access_key>\n",
    "    pip3 install pandas\n",
    "    ```\n",
    "\n",
    "\n",
    "4. Open **Capstone Project_ETL.py**, edit the `output_data` values in main() function with **your S3 URI** for collect the process's output and SAVE it.\n",
    "\n",
    "\n",
    "5. Upload/Create **config.cfg** and **Capstone Project_ETL.py** files to EMR Cluster\n",
    "\n",
    "\n",
    "6. Open **config.cfg**, edit the `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` values in \"[AWS]\" session with **AWS Access Key ID** and **AWS Secret Access Key**, respectively.\n",
    "\n",
    "\n",
    "7. Run script **Capstone Project_ETL.py** on the EMR Cluster for start to reads data from S3, processes that data using Spark, and writes them back to S3\n",
    "\n",
    "    ``` bash\n",
    "    /home/workspace# which spark-submit\n",
    "    /usr/bin/spark-submit\n",
    "    /home/workspace# /usr/bin/spark-submit --packages saurfang:spark-sas7bdat:2.0.0-s_2.11 --master yarn \"CapstoneProject_ETL.py\"\n",
    "    ```\n",
    "    \n",
    "8. After the script are suessful, you maybe need to terminate the AWS EMR Cluster. Please see AWS CLI for terminate the cluster as below:\n",
    "\n",
    "    ``` bash\n",
    "    aws emr terminate-clusters --cluster-ids <INPUT_CLUSTERID> --region us-west-2\n",
    "    \n",
    "    ```\n",
    "    \n",
    "> _INPUT_CLUSTERID_ : you can find the ClusterId from output message of create AWS EMR Cluster (Step 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    "\n",
    "_Run Quality Checks_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Prepare config and create spark session\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.cfg')\n",
    "config.sections()\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "spark2 = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Udacity_DataEngineering-CapstoneProject_Phakphoom\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.3\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID']) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY']) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\",\"false\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Integrity constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [cicid]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_df = spark2.read.parquet('s3a://capstoneproject-phakphoom/Capstone_Project/Datalake_Target/Immigration/*/*/*.parquet')\n",
    "immigration_df.createOrReplaceTempView(\"Immigration\")\n",
    "df_immigration_check = spark2.sql(\"\"\"\n",
    "                    SELECT cicid\n",
    "                    FROM Immigration WHERE cicid IS NULL \n",
    "                \"\"\")\n",
    "df_immigration_check.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [City]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_df = spark2.read.parquet('s3a://capstoneproject-phakphoom/Capstone_Project/Datalake_Target/Demographic/*.parquet')\n",
    "demographic_df.createOrReplaceTempView(\"Demographic\")\n",
    "df_demographic_check = spark2.sql(\"\"\"\n",
    "                    SELECT City \n",
    "                    FROM Demographic WHERE City IS NULL \n",
    "                \"\"\")\n",
    "df_demographic_check.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>City</th>\n",
       "      <th>Date_Local</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [No, City, Date_Local]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with active_session():\n",
    "    pollution_df = spark2.read.parquet('s3a://capstoneproject-phakphoom/Capstone_Project/Datalake_Target/Pollution/*/*/*.parquet')\n",
    "    #pollution_df = spark2.read.parquet('s3a://capstoneproject-phakphoom/Capstone_Project/Datalake_Target/Pollution/year=2016/*/*.parquet')\n",
    "    pollution_df.createOrReplaceTempView(\"Pollution\")\n",
    "    df_pollution_check = spark2.sql(\"\"\"\n",
    "                        SELECT No, City, Date_Local \n",
    "                        FROM Pollution WHERE No IS NULL OR City IS NULL OR Date_Local IS NULL\n",
    "                    \"\"\")\n",
    "    display(df_pollution_check.limit(10).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Record_Date</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Record_Date, City]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worldtemp_df = spark2.read.parquet('s3a://capstoneproject-phakphoom/Capstone_Project/Datalake_Target/WorldTemp/*.parquet')\n",
    "worldtemp_df.createOrReplaceTempView(\"WorldTemp\")\n",
    "df_worldtemp_check = spark2.sql(\"\"\"\n",
    "                    SELECT Record_Date, City \n",
    "                    FROM WorldTemp WHERE Record_Date IS NULL OR City IS NULL \n",
    "                \"\"\")\n",
    "df_worldtemp_check.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airline_ID</th>\n",
       "      <th>IATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Airline_ID, IATA]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_df = spark2.read.parquet('s3a://capstoneproject-phakphoom/Capstone_Project/Datalake_Target/Airline/*.parquet')\n",
    "airline_df.createOrReplaceTempView(\"Airline\")\n",
    "df_airline_check = spark2.sql(\"\"\"\n",
    "                    SELECT Airline_ID, IATA \n",
    "                    FROM Airline WHERE Airline_ID IS NULL OR IATA IS NULL \n",
    "                \"\"\")\n",
    "df_airline_check.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date_ref]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with active_session():\n",
    "    time_df = spark2.read.parquet('s3a://capstoneproject-phakphoom/Capstone_Project/Datalake_Target/Time/*/*/*.parquet')\n",
    "    time_df.createOrReplaceTempView(\"Time\")\n",
    "    df_time_check = spark2.sql(\"\"\"\n",
    "                        SELECT date_ref  \n",
    "                        FROM Time WHERE date_ref IS NULL \n",
    "                    \"\"\")\n",
    "    display(df_time_check.limit(10).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Data count records\n",
    "\n",
    "The data in \"Immigration\" table have 40790529 record(s)\n",
    "\n",
    "The data in \"Demographic\" table have 2891 record(s)\n",
    "\n",
    "The data in \"Pollution\" table have 436876 record(s)\n",
    "\n",
    "The data in \"WorldTemp\" table have 8235082 record(s)\n",
    "\n",
    "The data in \"Airline\" table have 1536 record(s)\n",
    "\n",
    "The data in \"Time\" table have 9112 record(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Summary records each tables *****\n",
      "The data in \"Immigration\" table have 40790529 record(s)\n",
      "The data in \"Demographic\" table have 2891 record(s)\n",
      "The data in \"Pollution\" table have 436876 record(s)\n",
      "The data in \"WorldTemp\" table have 8235082 record(s)\n",
      "The data in \"Airline\" table have 1536 record(s)\n",
      "The data in \"Time\" table have 9112 record(s)\n"
     ]
    }
   ],
   "source": [
    "with active_session():\n",
    "    print('\\n***** Summary records each tables *****')\n",
    "    immigration_df = spark2.read.parquet('s3a://capstoneproject-phakphoom/Capstone_Project/Datalake_Target/Immigration/*/*/*.parquet')\n",
    "    #immigration_df.createOrReplaceTempView(\"Immigration\")\n",
    "    df_immigration_count = immigration_df.count()\n",
    "    print('The data in \"{}\" table have {} record(s)'.format('Immigration', df_immigration_count))\n",
    "\n",
    "    demographic_df = spark2.read.parquet('s3a://capstoneproject-phakphoom/Capstone_Project/Datalake_Target/Demographic/*.parquet')\n",
    "    #demographic_df.createOrReplaceTempView(\"Demographic\")\n",
    "    df_demographic_count = demographic_df.count()\n",
    "    print('The data in \"{}\" table have {} record(s)'.format('Demographic', df_demographic_count))\n",
    "\n",
    "    pollution_df = spark2.read.parquet('s3a://capstoneproject-phakphoom/Capstone_Project/Datalake_Target/Pollution/*/*/*.parquet')\n",
    "    #pollution_df.createOrReplaceTempView(\"Pollution\")\n",
    "    df_pollution_count = pollution_df.count()\n",
    "    print('The data in \"{}\" table have {} record(s)'.format('Pollution', df_pollution_count))\n",
    "\n",
    "    worldtemp_df = spark2.read.parquet('s3a://capstoneproject-phakphoom/Capstone_Project/Datalake_Target/WorldTemp/*.parquet')\n",
    "    #worldtemp_df.createOrReplaceTempView(\"WorldTemp\")\n",
    "    df_worldtemp_check = worldtemp_df.count()\n",
    "    print('The data in \"{}\" table have {} record(s)'.format('WorldTemp', df_worldtemp_check))\n",
    "\n",
    "    airline_df = spark2.read.parquet('s3a://capstoneproject-phakphoom/Capstone_Project/Datalake_Target/Airline/*.parquet')\n",
    "    #airline_df.createOrReplaceTempView(\"Airline\")\n",
    "    df_airline_check = airline_df.count()\n",
    "    print('The data in \"{}\" table have {} record(s)'.format('Airline', df_airline_check))\n",
    "\n",
    "    time_df = spark2.read.parquet('s3a://capstoneproject-phakphoom/Capstone_Project/Datalake_Target/Time/*/*/*.parquet')\n",
    "    #time_df.createOrReplaceTempView(\"Time\")\n",
    "    df_time_count = time_df.count()\n",
    "    print('The data in \"{}\" table have {} record(s)'.format('Time', df_time_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.4 Query Example for Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>total_population</th>\n",
       "      <th>date_arr</th>\n",
       "      <th>date_dep</th>\n",
       "      <th>cntarr</th>\n",
       "      <th>cntdep</th>\n",
       "      <th>NO2_AQI</th>\n",
       "      <th>O3_AQI</th>\n",
       "      <th>SO2_AQI</th>\n",
       "      <th>CO_AQI</th>\n",
       "      <th>SUMM_IO</th>\n",
       "      <th>CUM_IO</th>\n",
       "      <th>Current_population</th>\n",
       "      <th>prev_NO2_AQI</th>\n",
       "      <th>diff_NO2_AQI</th>\n",
       "      <th>ROC_NO2_AQI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>531737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>None</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>84</td>\n",
       "      <td>147</td>\n",
       "      <td>531821</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-31.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>116</td>\n",
       "      <td>3</td>\n",
       "      <td>16.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>113</td>\n",
       "      <td>260</td>\n",
       "      <td>531934</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>None</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>107</td>\n",
       "      <td>367</td>\n",
       "      <td>532041</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-18.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>75</td>\n",
       "      <td>442</td>\n",
       "      <td>532116</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>61.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>91</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>89</td>\n",
       "      <td>531</td>\n",
       "      <td>532205</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>88</td>\n",
       "      <td>619</td>\n",
       "      <td>532293</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-08</td>\n",
       "      <td>2016-01-08</td>\n",
       "      <td>78</td>\n",
       "      <td>11</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>67</td>\n",
       "      <td>686</td>\n",
       "      <td>532360</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-09</td>\n",
       "      <td>2016-01-09</td>\n",
       "      <td>92</td>\n",
       "      <td>16</td>\n",
       "      <td>21.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>76</td>\n",
       "      <td>762</td>\n",
       "      <td>532436</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-27.586207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-10</td>\n",
       "      <td>2016-01-10</td>\n",
       "      <td>93</td>\n",
       "      <td>15</td>\n",
       "      <td>18.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>78</td>\n",
       "      <td>840</td>\n",
       "      <td>532514</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-14.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-11</td>\n",
       "      <td>2016-01-11</td>\n",
       "      <td>121</td>\n",
       "      <td>5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>116</td>\n",
       "      <td>956</td>\n",
       "      <td>532630</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>38.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-12</td>\n",
       "      <td>2016-01-12</td>\n",
       "      <td>99</td>\n",
       "      <td>20</td>\n",
       "      <td>28.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>79</td>\n",
       "      <td>1035</td>\n",
       "      <td>532709</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>96</td>\n",
       "      <td>25</td>\n",
       "      <td>28.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>71</td>\n",
       "      <td>1106</td>\n",
       "      <td>532780</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>66</td>\n",
       "      <td>18</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>48</td>\n",
       "      <td>1154</td>\n",
       "      <td>532828</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-15</td>\n",
       "      <td>2016-01-15</td>\n",
       "      <td>108</td>\n",
       "      <td>16</td>\n",
       "      <td>27.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>92</td>\n",
       "      <td>1246</td>\n",
       "      <td>532920</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-6.896552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-16</td>\n",
       "      <td>2016-01-16</td>\n",
       "      <td>69</td>\n",
       "      <td>19</td>\n",
       "      <td>29.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>50</td>\n",
       "      <td>1296</td>\n",
       "      <td>532970</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.407407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>82</td>\n",
       "      <td>8</td>\n",
       "      <td>25.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>74</td>\n",
       "      <td>1370</td>\n",
       "      <td>533044</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-13.793103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>97</td>\n",
       "      <td>19</td>\n",
       "      <td>34.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>78</td>\n",
       "      <td>1448</td>\n",
       "      <td>533122</td>\n",
       "      <td>25.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-19</td>\n",
       "      <td>2016-01-19</td>\n",
       "      <td>41</td>\n",
       "      <td>10</td>\n",
       "      <td>25.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31</td>\n",
       "      <td>1479</td>\n",
       "      <td>533153</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-26.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-20</td>\n",
       "      <td>2016-01-20</td>\n",
       "      <td>49</td>\n",
       "      <td>16</td>\n",
       "      <td>34.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>33</td>\n",
       "      <td>1512</td>\n",
       "      <td>533186</td>\n",
       "      <td>25.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-21</td>\n",
       "      <td>2016-01-21</td>\n",
       "      <td>53</td>\n",
       "      <td>20</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>33</td>\n",
       "      <td>1545</td>\n",
       "      <td>533219</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-14.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>83</td>\n",
       "      <td>43</td>\n",
       "      <td>36.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>40</td>\n",
       "      <td>1585</td>\n",
       "      <td>533259</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24.137931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-23</td>\n",
       "      <td>2016-01-23</td>\n",
       "      <td>68</td>\n",
       "      <td>23</td>\n",
       "      <td>42.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>45</td>\n",
       "      <td>1630</td>\n",
       "      <td>533304</td>\n",
       "      <td>36.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-24</td>\n",
       "      <td>2016-01-24</td>\n",
       "      <td>73</td>\n",
       "      <td>15</td>\n",
       "      <td>30.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>58</td>\n",
       "      <td>1688</td>\n",
       "      <td>533362</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-28.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-25</td>\n",
       "      <td>2016-01-25</td>\n",
       "      <td>59</td>\n",
       "      <td>21</td>\n",
       "      <td>26.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>38</td>\n",
       "      <td>1726</td>\n",
       "      <td>533400</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-13.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-26</td>\n",
       "      <td>2016-01-26</td>\n",
       "      <td>48</td>\n",
       "      <td>10</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>38</td>\n",
       "      <td>1764</td>\n",
       "      <td>533438</td>\n",
       "      <td>26.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>34.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-27</td>\n",
       "      <td>2016-01-27</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>30.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1772</td>\n",
       "      <td>533446</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-14.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>65</td>\n",
       "      <td>25</td>\n",
       "      <td>25.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>40</td>\n",
       "      <td>1812</td>\n",
       "      <td>533486</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-16.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-29</td>\n",
       "      <td>2016-01-29</td>\n",
       "      <td>50</td>\n",
       "      <td>39</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1823</td>\n",
       "      <td>533497</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-30</td>\n",
       "      <td>2016-01-30</td>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "      <td>23.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17</td>\n",
       "      <td>1840</td>\n",
       "      <td>533514</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-20.689655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>TUCSON</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>531674</td>\n",
       "      <td>2016-01-31</td>\n",
       "      <td>2016-01-31</td>\n",
       "      <td>70</td>\n",
       "      <td>19</td>\n",
       "      <td>11.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>51</td>\n",
       "      <td>1891</td>\n",
       "      <td>533565</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-52.173913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      city    state  total_population    date_arr    date_dep  cntarr  cntdep  \\\n",
       "0   TUCSON  ARIZONA            531674  2016-01-01        None      63       0   \n",
       "1   TUCSON  ARIZONA            531674  2016-01-02        None      84       0   \n",
       "2   TUCSON  ARIZONA            531674  2016-01-03  2016-01-03     116       3   \n",
       "3   TUCSON  ARIZONA            531674  2016-01-04        None     107       0   \n",
       "4   TUCSON  ARIZONA            531674  2016-01-05  2016-01-05      80       5   \n",
       "5   TUCSON  ARIZONA            531674  2016-01-06  2016-01-06      91       2   \n",
       "6   TUCSON  ARIZONA            531674  2016-01-07  2016-01-07      91       3   \n",
       "7   TUCSON  ARIZONA            531674  2016-01-08  2016-01-08      78      11   \n",
       "8   TUCSON  ARIZONA            531674  2016-01-09  2016-01-09      92      16   \n",
       "9   TUCSON  ARIZONA            531674  2016-01-10  2016-01-10      93      15   \n",
       "10  TUCSON  ARIZONA            531674  2016-01-11  2016-01-11     121       5   \n",
       "11  TUCSON  ARIZONA            531674  2016-01-12  2016-01-12      99      20   \n",
       "12  TUCSON  ARIZONA            531674  2016-01-13  2016-01-13      96      25   \n",
       "13  TUCSON  ARIZONA            531674  2016-01-14  2016-01-14      66      18   \n",
       "14  TUCSON  ARIZONA            531674  2016-01-15  2016-01-15     108      16   \n",
       "15  TUCSON  ARIZONA            531674  2016-01-16  2016-01-16      69      19   \n",
       "16  TUCSON  ARIZONA            531674  2016-01-17  2016-01-17      82       8   \n",
       "17  TUCSON  ARIZONA            531674  2016-01-18  2016-01-18      97      19   \n",
       "18  TUCSON  ARIZONA            531674  2016-01-19  2016-01-19      41      10   \n",
       "19  TUCSON  ARIZONA            531674  2016-01-20  2016-01-20      49      16   \n",
       "20  TUCSON  ARIZONA            531674  2016-01-21  2016-01-21      53      20   \n",
       "21  TUCSON  ARIZONA            531674  2016-01-22  2016-01-22      83      43   \n",
       "22  TUCSON  ARIZONA            531674  2016-01-23  2016-01-23      68      23   \n",
       "23  TUCSON  ARIZONA            531674  2016-01-24  2016-01-24      73      15   \n",
       "24  TUCSON  ARIZONA            531674  2016-01-25  2016-01-25      59      21   \n",
       "25  TUCSON  ARIZONA            531674  2016-01-26  2016-01-26      48      10   \n",
       "26  TUCSON  ARIZONA            531674  2016-01-27  2016-01-27      38      30   \n",
       "27  TUCSON  ARIZONA            531674  2016-01-28  2016-01-28      65      25   \n",
       "28  TUCSON  ARIZONA            531674  2016-01-29  2016-01-29      50      39   \n",
       "29  TUCSON  ARIZONA            531674  2016-01-30  2016-01-30      36      19   \n",
       "30  TUCSON  ARIZONA            531674  2016-01-31  2016-01-31      70      19   \n",
       "\n",
       "    NO2_AQI  O3_AQI  SO2_AQI  CO_AQI  SUMM_IO  CUM_IO  Current_population  \\\n",
       "0      29.0    27.0      0.0     7.0       63      63              531737   \n",
       "1      20.0    28.0      0.0     6.0       84     147              531821   \n",
       "2      16.0    28.0      0.0     5.0      113     260              531934   \n",
       "3      13.0    22.0      0.0     5.0      107     367              532041   \n",
       "4      21.0    30.0      0.0     5.0       75     442              532116   \n",
       "5      25.0    36.0      0.0     5.0       89     531              532205   \n",
       "6      29.0    30.0      0.0     5.0       88     619              532293   \n",
       "7      29.0    31.0      0.0     6.0       67     686              532360   \n",
       "8      21.0    31.0      0.0     7.0       76     762              532436   \n",
       "9      18.0    26.0      0.0     6.0       78     840              532514   \n",
       "10     25.0    30.0      0.0     6.0      116     956              532630   \n",
       "11     28.0    27.0      0.0     6.0       79    1035              532709   \n",
       "12     28.0    27.0      0.0     6.0       71    1106              532780   \n",
       "13     29.0    30.0      0.0     6.0       48    1154              532828   \n",
       "14     27.0    31.0      0.0     5.0       92    1246              532920   \n",
       "15     29.0    32.0      0.0     5.0       50    1296              532970   \n",
       "16     25.0    31.0      0.0     5.0       74    1370              533044   \n",
       "17     34.0    22.0      0.0     6.0       78    1448              533122   \n",
       "18     25.0    31.0      0.0     6.0       31    1479              533153   \n",
       "19     34.0    32.0      0.0     5.0       33    1512              533186   \n",
       "20     29.0    29.0      0.0     6.0       33    1545              533219   \n",
       "21     36.0    35.0      0.0     6.0       40    1585              533259   \n",
       "22     42.0    29.0      0.0     7.0       45    1630              533304   \n",
       "23     30.0    39.0      0.0     8.0       58    1688              533362   \n",
       "24     26.0    35.0      0.0     5.0       38    1726              533400   \n",
       "25     35.0    35.0      0.0     5.0       38    1764              533438   \n",
       "26     30.0    35.0      0.0     6.0        8    1772              533446   \n",
       "27     25.0    27.0      1.0     5.0       40    1812              533486   \n",
       "28     29.0    31.0      0.0     5.0       11    1823              533497   \n",
       "29     23.0    38.0      0.0     6.0       17    1840              533514   \n",
       "30     11.0    35.0      0.0     5.0       51    1891              533565   \n",
       "\n",
       "    prev_NO2_AQI  diff_NO2_AQI  ROC_NO2_AQI  \n",
       "0            NaN           0.0     0.000000  \n",
       "1           29.0          -9.0   -31.034483  \n",
       "2           20.0          -4.0   -20.000000  \n",
       "3           16.0          -3.0   -18.750000  \n",
       "4           13.0           8.0    61.538462  \n",
       "5           21.0           4.0    19.047619  \n",
       "6           25.0           4.0    16.000000  \n",
       "7           29.0           0.0     0.000000  \n",
       "8           29.0          -8.0   -27.586207  \n",
       "9           21.0          -3.0   -14.285714  \n",
       "10          18.0           7.0    38.888889  \n",
       "11          25.0           3.0    12.000000  \n",
       "12          28.0           0.0     0.000000  \n",
       "13          28.0           1.0     3.571429  \n",
       "14          29.0          -2.0    -6.896552  \n",
       "15          27.0           2.0     7.407407  \n",
       "16          29.0          -4.0   -13.793103  \n",
       "17          25.0           9.0    36.000000  \n",
       "18          34.0          -9.0   -26.470588  \n",
       "19          25.0           9.0    36.000000  \n",
       "20          34.0          -5.0   -14.705882  \n",
       "21          29.0           7.0    24.137931  \n",
       "22          36.0           6.0    16.666667  \n",
       "23          42.0         -12.0   -28.571429  \n",
       "24          30.0          -4.0   -13.333333  \n",
       "25          26.0           9.0    34.615385  \n",
       "26          35.0          -5.0   -14.285714  \n",
       "27          30.0          -5.0   -16.666667  \n",
       "28          25.0           4.0    16.000000  \n",
       "29          29.0          -6.0   -20.689655  \n",
       "30          23.0         -12.0   -52.173913  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with active_session():\n",
    "    immigration_df = spark2.read.parquet('s3a://capstoneproject-phakphoom/Capstone_Project/Datalake_Target/Immigration/year=2016/month=1/*.parquet')\n",
    "    immigration_df.createOrReplaceTempView(\"Immigration\")\n",
    "\n",
    "    demographic_df = spark2.read.parquet('s3a://capstoneproject-phakphoom/Capstone_Project/Datalake_Target/Demographic/*.parquet')\n",
    "    demographic_df.createOrReplaceTempView(\"Demographic\")\n",
    "\n",
    "    pollution_df = spark2.read.parquet('s3a://capstoneproject-phakphoom/Capstone_Project/Datalake_Target/Pollution/year=2016/month=1/*.parquet')\n",
    "    pollution_df.createOrReplaceTempView(\"Pollution\")\n",
    "\n",
    "    df_query1 = spark2.sql(\"\"\"\n",
    "\n",
    "        SELECT *, \n",
    "            cntarr - cntdep AS SUMM_IO\n",
    "            /*,(total_population + cntarr) - cntdep AS Current_population*/\n",
    "        FROM \n",
    "        (\n",
    "            SELECT Demog.city, Demog.state, INT(Demog.total_population) AS total_population, \n",
    "                    Immig.date_arr, Immig.date_dep, \n",
    "                        CASE WHEN Immig.cntarr IS NULL THEN INT(0) ELSE INT(Immig.cntarr) END AS cntarr,\n",
    "                        CASE WHEN Immig.cntdep IS NULL THEN INT(0) ELSE INT(Immig.cntdep) END AS cntdep,\n",
    "                    Pollu.NO2_AQI, Pollu.O3_AQI, Pollu.SO2_AQI, Pollu.CO_AQI\n",
    "            FROM\n",
    "                (SELECT city, state, total_population FROM Demographic GROUP BY city, state, total_population) AS Demog\n",
    "                JOIN\n",
    "                (\n",
    "                    SELECT date_arr, date_dep, immigration_arr.i94port_city, immigration_arr.i94port_statecode, cntarr, cntdep\n",
    "                    FROM \n",
    "                        (SELECT arrdate_todate as date_arr, \n",
    "                            i94port, i94port_city, i94port_statecode,\n",
    "                            COUNT(*) as cntarr\n",
    "                        FROM Immigration \n",
    "                        /*WHERE arrdate_todate = '2016-01-01'*/\n",
    "                        GROUP BY arrdate_todate, i94port, i94port_city, i94port_statecode) as immigration_arr\n",
    "                    FULL OUTER JOIN \n",
    "                        (SELECT depdate_todate as date_dep, \n",
    "                            i94port, i94port_city, i94port_statecode,\n",
    "                            COUNT(*) as cntdep\n",
    "                        FROM Immigration \n",
    "                        /*WHERE depdate_todate = '2016-01-01' */\n",
    "                        GROUP BY depdate_todate, i94port, i94port_city, i94port_statecode) as immigration_dep\n",
    "                    ON date_arr = date_dep AND immigration_arr.i94port = immigration_dep.i94port \n",
    "                ) AS Immig\n",
    "                ON Demog.city = Immig.i94port_city\n",
    "                JOIN\n",
    "                (\n",
    "                    SELECT City, Date_Local, AVG(NO2_AQI) AS NO2_AQI, AVG(O3_AQI) AS O3_AQI, AVG(SO2_AQI) AS SO2_AQI, AVG(CO_AQI) AS CO_AQI \n",
    "                    FROM Pollution GROUP BY City, Date_Local\n",
    "                ) as Pollu \n",
    "                ON Demog.city = Pollu.City AND Immig.i94port_city = Pollu.City AND Immig.date_arr = Pollu.Date_Local\n",
    "        ) AS MAIN\n",
    "        WHERE MAIN.city = 'TUCSON' \n",
    "        ORDER BY MAIN.date_arr \n",
    "\n",
    "        \"\"\")\n",
    "\n",
    "    windowval = (Window.partitionBy('city').orderBy('date_arr')\n",
    "                 .rangeBetween(Window.unboundedPreceding, 0))\n",
    "    df_query1_cumsum = df_query1.withColumn('CUM_IO', F.sum('SUMM_IO').over(windowval))\n",
    "    df_query1_cumsum_totalpopu = df_query1_cumsum.withColumn('Current_population', df_query1_cumsum.total_population + df_query1_cumsum.CUM_IO)\n",
    "\n",
    "    \n",
    "    pollution_window = Window.partitionBy(\"city\").orderBy(\"date_arr\")\n",
    "    df_query1_cumsum_totalpopu_pollu = df_query1_cumsum_totalpopu.withColumn(\"prev_NO2_AQI\", F.lag(df_query1_cumsum_totalpopu.NO2_AQI).over(pollution_window))\n",
    "    df_query1_cumsum_totalpopu_pollu = df_query1_cumsum_totalpopu_pollu.withColumn(\"diff_NO2_AQI\", F.when(F.isnull(df_query1_cumsum_totalpopu_pollu.NO2_AQI - df_query1_cumsum_totalpopu_pollu.prev_NO2_AQI), 0) \\\n",
    "                                                                    .otherwise(df_query1_cumsum_totalpopu_pollu.NO2_AQI - df_query1_cumsum_totalpopu_pollu.prev_NO2_AQI))\n",
    "    df_query1_cumsum_totalpopu_pollu = df_query1_cumsum_totalpopu_pollu.withColumn(\"ROC_NO2_AQI\", F.when(F.isnull(df_query1_cumsum_totalpopu_pollu.diff_NO2_AQI / df_query1_cumsum_totalpopu_pollu.prev_NO2_AQI * 100), 0) \\\n",
    "                                                                    .otherwise(df_query1_cumsum_totalpopu_pollu.diff_NO2_AQI / df_query1_cumsum_totalpopu_pollu.prev_NO2_AQI * 100)) \n",
    "\n",
    "    \n",
    "    display(df_query1_cumsum_totalpopu_pollu.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.5 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Immigration table**\n",
    "\t\n",
    "* cicid: double (nullable = true) : unique number for the immigrants\n",
    "* i94yr: double (nullable = true) : 4 digit year\n",
    "* i94mon: double (nullable = true) : Numeric month\n",
    "* i94cit: string (nullable = true) : Citizenship\n",
    "* i94res: string (nullable = true) : Residence\n",
    "* i94port: string (nullable = true) : City, State code\n",
    "* i94port_city: string (nullable = true) : City\n",
    "* i94port_statecode: string (nullable = true) : State code\n",
    "* arrdate_todate: string (nullable = true) : Arrival Date in the U.S.\n",
    "* i94mode: string (nullable = true) : Travel by Land, Air , and Sea\n",
    "* i94addr: string (nullable = true) : is where the immigrants resides in U.S.\n",
    "* depdate_todate: string (nullable = true) : Departure Date from the U.S.\n",
    "* i94bir: double (nullable = true) : Age of Respondent in Years\n",
    "* i94visa: string (nullable = true) : Visa categories (Business, Pleasure, and Student)\n",
    "* biryear: double (nullable = true) : 4 digit year of birth\n",
    "* gender: string (nullable = true) : Non-immigrant sex\n",
    "* airline: string (nullable = true) : Airline used to arrive in U.S.\n",
    "* fltno: string (nullable = true) : Flight number of Airline used to arrive in U.S.\n",
    " \n",
    "**WorldTemp table**\n",
    "\n",
    "* Record_Date: string (nullable = true) : Date of records\n",
    "* AverageTemperature: string (nullable = true) : Average Temperature\n",
    "* AverageTemperatureUncertainty: string (nullable = true) : Average Temperature Uncertainty\n",
    "* City: string (nullable = true) : City\n",
    "* Country: string (nullable = true) : Country\n",
    "\n",
    "**Demographic table**\n",
    "\n",
    "* City: string (nullable = true) : City\n",
    "* State: string (nullable = true) : State\n",
    "* Median_Age: string (nullable = true) : Median Age of Population\n",
    "* Male_Population: string (nullable = true) : Male Population\n",
    "* Female_Population: string (nullable = true) : Female Population\n",
    "* Total_Population: string (nullable = true) : Total Population\n",
    "* NumberofVeterans: string (nullable = true) : Number of Veterans\n",
    "* Foreign_born: string (nullable = true) : Foreign-born\n",
    "* Average_Household_Size: string (nullable = true) : Average Household Size\n",
    "* State_Code: string (nullable = true) : State code\n",
    "* Race: string (nullable = true) : Race\n",
    "* Count: string (nullable = true) : Count by Race\n",
    "\n",
    "**Pollution table**\n",
    "\n",
    "* No: string (nullable = true) : Number of records\n",
    "* State_Code: string (nullable = true) : State Code\n",
    "* County_Code: string (nullable = true) : County Code\n",
    "* Site_Num: string (nullable = true) : Site Num\n",
    "* Address: string (nullable = true) : Address\n",
    "* State: string (nullable = true) : State\n",
    "* County: string (nullable = true) : County\n",
    "* City: string (nullable = true) : City\n",
    "* Date_Local: string (nullable = true) : Date of records\n",
    "* NO2_Units: string (nullable = true) : Units of NO2\n",
    "* NO2_Mean: double (nullable = true) : Mean of NO2\n",
    "* NO2_1st_Max_Value: double (nullable = true) = 1st Max Value of NO2\n",
    "* NO2_1st_Max_Hour: double (nullable = true) = 1st Max Hour of NO2\n",
    "* NO2_AQI: double (nullable = true) = AQI of NO2\n",
    "* O3_Units: string (nullable = true) : Units of O3\n",
    "* O3_Mean: double (nullable = true) : Mean of O3\n",
    "* O3_1st_Max_Value: double (nullable = true) = 1st Max Value of O3\n",
    "* O3_1st_Max_Hour: double (nullable = true) = 1st Max Hour of O3\n",
    "* O3_AQI: double (nullable = true) = AQI of O3\n",
    "* SO2_Units: string (nullable = true) : Units of SO2\n",
    "* SO2_Mean: double (nullable = true) : Mean of SO2\n",
    "* SO2_1st_Max_Value: double (nullable = true) = 1st Max Value of SO2\n",
    "* SO2_1st_Max_Hour: double (nullable = true) = 1st Max Hour of SO2\n",
    "* SO2_AQI: double (nullable = true) = AQI of \n",
    "* CO_Units: string (nullable = true) : Units of CO\n",
    "* CO_Mean: double (nullable = true) :  : Mean of CO\n",
    "* CO_1st_Max_Value: double (nullable = true) = 1st Max Value of CO\n",
    "* CO_1st_Max_Hour: double (nullable = true) = 1st Max Hour of CO\n",
    "* CO_AQI: double (nullable = true) = AQI of CO\n",
    "\n",
    "**Airline table**\n",
    "\n",
    "* Airline_ID: string (nullable = true) : Unique OpenFlights identifier for this airline\n",
    "* Name: string (nullable = true) : Name of the airline.\n",
    "* Alias: string (nullable = true) : Alias of the airline. For example, All Nippon Airways is commonly known as \"ANA\"\n",
    "* IATA: string (nullable = true) : 2-letter IATA code, if available\n",
    "* ICAO: string (nullable = true) : 3-letter ICAO code, if available\n",
    "* Callsign: string (nullable = true) : Airline callsign\n",
    "* Country: string (nullable = true) : Country or territory where airport is located\n",
    "* Active: string (nullable = true) : \"Y\" if the airline is or has until recently been operational, \"N\" if it is defunct. This field is not reliable: in particular, \n",
    "             major airlines that stopped flying long ago, but have not had their IATA code reassigned (eg. Ansett/AN), will incorrectly show as \"Y\".\n",
    "\n",
    "**Time table**\n",
    "\n",
    "* date_ref: string (nullable = true) : Date are referenced in Data Model\n",
    "* day: integer (nullable = true) : Day of month\n",
    "* week: integer (nullable = true) : Week of year\n",
    "* month: integer (nullable = true) : Month of year\n",
    "* year: integer (nullable = true) : 4 digit year\n",
    "* weekday: integer (nullable = true) : Day of week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "> In this project, I used Python language, Apache Spark, and Data Lakes technology running on AWS with S3 and EMR.\n",
    "The main reason is that Data Lakes support Unstructured data which in this project uses 2 types of data files: sas7bdat and CSV.\n",
    "Next, it is possible to Data Analysis without inserting into a pre-defined schema/table, also known as \"Schema-On-Read\".\n",
    "It's can reduce the time to analyze the data considerably. This can be further applied for predictive analytics or machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Propose how often the data should be updated and why.\n",
    "\n",
    "> Our Data Models should use at least Daily or Monthly data to update the data. Since the analysis uses Historical Data, the more data the model is, the more efficient the model is, and predictive analytics or machine learning will produce more accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " \n",
    " > For the Infrastructure side, it needs to consider the scale up and scale out of the AWS EMR cluster to support more data/sources.\n",
    "On the Data side, there must be steps to do Data Quality & Cleansing in order to get the most efficient data.\n",
    " \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " \n",
    " > First, add/edit processes to work effectively with Daily Data, such as correct ETL Process, setting Job Schedules, Data Quality, and process of verifying data with Daily Data received and data after processing to ensure that it is correct. and reliable.\n",
    " \n",
    " * The database needed to be accessed by 100+ people.\n",
    " \n",
    " > Data Lakes may not be suitable for mass user access, which requires adding a Pipeline to Load processed data to Relational Table. Therefore, consider using an RDBMS such as AWS Redshift that has the ability to support heavy concurrent usage, suitable for both OLTP and OLAB Workload.\n",
    "It also supports Scaling to be able to support a large number of users.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Reference:\n",
    "\n",
    "I have been searching for some ideas or other help to find solutions in my code that are shown as below:\n",
    "    \n",
    "1. Pandas read csv() Tutorial: Importing Data ... [Link](https://knowledge.udacity.com/questions/368715)\n",
    "2. How to add header row to a pandas DataFrame ... [Link](https://stackoverflow.com/questions/34091877/how-to-add-header-row-to-a-pandas-dataframe)\n",
    "3. Pandas – Read only the first n rows of a CSV file ... [Link](https://knowledge.udacity.com/questions/368715)\n",
    "4. Find Minimum, Maximum, and Average Value of PySpark Dataframe column ... [Link](https://www.geeksforgeeks.org/find-minimum-maximum-and-average-value-of-pyspark-dataframe-column/)\n",
    "5. MAXIMUM OR MINIMUM VALUE OF COLUMN IN PYSPARK ... [Link](https://www.datasciencemadesimple.com/maximum-or-minimum-value-of-column-in-pyspark/)\n",
    "6. Best way to get the max value in a Spark dataframe column ... [Link](https://stackoverflow.com/questions/33224740/best-way-to-get-the-max-value-in-a-spark-dataframe-column)\n",
    "7. PySpark Read CSV file into DataFrame ... [Link](https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/#header)\n",
    "8. Spark 2.4 CSV Load Issue with option \"nullvalue\" ... [Link](https://stackoverflow.com/questions/56752408/spark-2-4-csv-load-issue-with-option-nullvalue)\n",
    "9. How to Replace a String in Spark DataFrame | Spark Scenario Based Question ... [Link](https://www.learntospark.com/2021/03/replace-a-string-in-spark.html)\n",
    "10. How to Replace Null Values in Spark DataFrames ... [Link](https://towardsdatascience.com/how-to-replace-null-values-in-spark-dataframes-ab183945b57d)\n",
    "11. Navigating None and null in PySpark ... [Link](https://mungingdata.com/pyspark/none-null/)\n",
    "12. Create Spark SQL isdate Function – Date Validation ... [Link](https://dwgeek.com/create-spark-sql-isdate-function-date-validation.html/)\n",
    "13. How to Write Spark UDFs (User Defined Functions) in Python ... [Link](https://www.bmc.com/blogs/how-to-write-spark-udf-python/)\n",
    "14. The Ultimate Guide to Data Cleaning ... [Link](https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4)\n",
    "15. Python: The Boolean confusion ... [Link](https://towardsdatascience.com/python-the-boolean-confusion-f7fc5288f0ce)\n",
    "16. Column definitions of immigration data ... [Link](https://knowledge.udacity.com/questions/185453)\n",
    "17. How i94yr and i94mon columns are related to depdate and arrdate columns in I94 immigration ?. ... [Link](https://knowledge.udacity.com/questions/297018)\n",
    "18. Matching i94port (I94 Immigration Dataset) to local_code (Airport Codes Dataset) ... [Link](https://knowledge.udacity.com/questions/613179)\n",
    "19. SAS date format conversion in spark ... [Link](https://knowledge.udacity.com/questions/741863)\n",
    "20. Trying to convert a SAS time format to Pandas datetime ... [Link](https://knowledge.udacity.com/questions/783322)\n",
    "21. How to convert SAS numeric date to Date data type in SQL? ... [Link](https://knowledge.udacity.com/questions/381099)\n",
    "22. what is cicid in immigration file ... [Link](https://knowledge.udacity.com/questions/709930)\n",
    "23. Column definitions of immigration data ... [Link](https://knowledge.udacity.com/questions/185453)\n",
    "24. I am confused by what the i94addr represents, could you please explain how it should be used ... [Link](https://knowledge.udacity.com/questions/418299)\n",
    "25. Not clear what they are asking with \\\"Integrity constraints on the relational database (e.g., unique key, data type, etc.)\\\" ... [Link](https://knowledge.udacity.com/questions/767640)\n",
    "26. Failed to find data source: com.github.saurfang.sas.spark error ... [Link](https://knowledge.udacity.com/questions/224269)\n",
    "27. Can not read metadata from sas7bdat file ... [Link](https://knowledge.udacity.com/questions/386959)\n",
    "28. ModuleNotFoundError: No module named 'pandas' ... [Link](http://net-informations.com/ds/err/pderr.htm)\n",
    "29. Reading in SAS files from S3 ... [Link](https://knowledge.udacity.com/questions/197139)\n",
    "30. How to keep SSH connections alive ... [Link](https://www.a2hosting.com/kb/getting-started-guide/accessing-your-account/keeping-ssh-connections-alive)\n",
    "31. I am trying load data into S3 for Capstone project but I get error - Exception: Java gateway process exited before sending its port number ... [Link](https://knowledge.udacity.com/questions/703078)\n",
    "32. Read a sas7bdat as Spark Dataframe ... [Link](https://knowledge.udacity.com/questions/72864)\n",
    "33. Reading and appending files into a spark dataframe ... [Link](https://stackoverflow.com/questions/57824016/reading-and-appending-files-into-a-spark-dataframe)\n",
    "34. Python Pandas Read multiple SAS files from a list into separate dataframes ... [Link](https://stackoverflow.com/questions/52894787/python-pandas-read-multiple-sas-files-from-a-list-into-separate-dataframes)\n",
    "35. Read all the SAS files in the folder to a single dataframe in a single load. ... [Link](https://knowledge.udacity.com/questions/280093)\n",
    "36. Read Immigration Dataset in SAS format AND Write as parquet format on S3. ... [Link](https://github.com/MaulikDave9/DEND-Immigration-2016/blob/f3f00480a0d438878280a03007102341b75f5cf7/Miscellaneous/Test_SparkRead_I94SAS.ipynb)\n",
    "37. How to read multiple .sas7bdat files into data frame ?.. ... [Link](https://knowledge.udacity.com/questions/292327)\n",
    "38. Reading in SAS files from S3 ... [Link](https://knowledge.udacity.com/questions/197139)\n",
    "39. Spark SQL Join Types with examples ... [Link](https://sparkbyexamples.com/spark/spark-sql-dataframe-join/)\n",
    "40. Spark vs NoSQL:Is (Hadoop or S3(for storage)+Spark(for querying)) combination more powerful than NoSQL like Cassandra as we can join tables in spark ? ... [Link](https://knowledge.udacity.com/questions/708563)\n",
    "41. How to calculate cumulative sum using sqlContext ... [Link](https://stackoverflow.com/questions/34726268/how-to-calculate-cumulative-sum-using-sqlcontext)\n",
    "42. How to get cumulative sum ... [Link](https://stackoverflow.com/questions/2120544/how-to-get-cumulative-sum)\n",
    "43. EMR notebook fails because JupyterEnterpriseGateway application not installed on existing cluster AWS [closed] ... [Link](https://stackoverflow.com/questions/66729542/emr-notebook-fails-because-jupyterenterprisegateway-application-not-installed-on)\n",
    "44. create-cluster ... [Link](https://docs.aws.amazon.com/cli/latest/reference/emr/create-cluster.html)\n",
    "45. PySpark and SparkSQL Basics ... [Link](https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53)\n",
    "46. PySpark Window Functions ... [Link](https://sparkbyexamples.com/pyspark/pyspark-window-functions/#aggregate-functions)\n",
    "47. SQL PARTITION BY Clause overview ... [Link](https://www.sqlshack.com/sql-partition-by-clause-overview/)\n",
    "48. Unable to use PARTITION BY with COUNT(*) when multiple columns are used ... [Link](https://stackoverflow.com/questions/35026644/unable-to-use-partition-by-with-count-when-multiple-columns-are-used)\n",
    "49. Demographics dataset 'count' column ... [Link](https://knowledge.udacity.com/questions/319686)\n",
    "50. Splitting a string in SparkSQL ... [Link](https://stackoverflow.com/questions/44688168/splitting-a-string-in-sparksql)\n",
    "51. Spark split() function to convert string to Array column ... [Link](https://sparkbyexamples.com/spark/convert-delimiter-separated-string-to-array-column-in-spark/#:~:text=Spark%20SQL%20provides%20split(),e.t.c%2C%20and%20converting%20into%20ArrayType.)\n",
    "52. Spark SQL String Functions Explained ... [Link](https://sparkbyexamples.com/spark/usage-of-spark-sql-string-functions/)\n",
    "53. Spark Docs » Functions ... [Link](https://spark.apache.org/docs/2.3.0/api/sql/index.html)\n",
    "54. PySpark Convert String Type to Double Type ... [Link](https://sparkbyexamples.com/pyspark/pyspark-convert-string-type-to-double-type-float-type/)\n",
    "55. PySpark Drop Rows with NULL or None Values ... [Link](https://sparkbyexamples.com/pyspark/pyspark-drop-rows-with-null-values/)\n",
    "56. PySpark Groupby Explained with Example ... [Link](https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example/)\n",
    "57. SQL - AVG Function ... [Link](https://www.tutorialspoint.com/sql/sql-avg-function.htm#:~:text=SQL%20AVG%20function%20is%20used,a%20field%20in%20various%20records.&text=You%20can%20take%20average%20of,typed%20pages%20by%20every%20person.)\n",
    "58. Spark Cast String Type to Integer Type (int) ... [Link](https://sparkbyexamples.com/spark/spark-cast-string-type-to-integer-type-int/#:~:text=In%20Spark%20SQL%2C%20in%20order,selectExpr()%20and%20SQL%20expression.)\n",
    "59. PySpark to_date() – Convert String to Date Format ... [Link](https://sparkbyexamples.com/pyspark/pyspark-to_date-convert-string-to-date-format/)\n",
    "60. Python Spark Cumulative Sum by Group Using DataFrame ... [Link](https://stackoverflow.com/questions/45946349/python-spark-cumulative-sum-by-group-using-dataframe)\n",
    "61. Check Type: How to check if something is a RDD or a DataFrame? ... [Link](https://stackoverflow.com/questions/36731365/check-type-how-to-check-if-something-is-a-rdd-or-a-dataframe)\n",
    "62. AttributeError: ‘DataFrame’ object has no attribute ‘map’ in PySpark ... [Link](https://sparkbyexamples.com/pyspark/attributeerror-dataframe-object-has-no-attribute-map-in-pyspark/)\n",
    "63. ValueError: Cannot convert column into bool ... [Link](https://stackoverflow.com/questions/48282321/valueerror-cannot-convert-column-into-bool)\n",
    "64. understanding level =0 and group_keys ... [Link](https://stackoverflow.com/questions/49859182/understanding-level-0-and-group-keys)\n",
    "65. How to calculate top 5 max values in Pyspark ... [Link](https://www.learneasysteps.com/how-to-calculate-top-5-max-values-in-pyspark/)\n",
    "66. Comprehensive Guide to Grouping and Aggregating with Pandas ... [Link](https://pbpython.com/groupby-agg.html)\n",
    "67. Pandas .groupby(), Lambda Functions, & Pivot Tables ... [Link](https://mode.com/python-tutorial/pandas-groupby-and-python-lambda-functions/)\n",
    "68. pandas.DataFrame.groupby ... [Link](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html)\n",
    "69. Pandas groupby() Syntax ... [Link](https://sparkbyexamples.com/pandas/pandas-groupby-explained-with-examples/)\n",
    "70. Indexing and selecting data ... [Link](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html)\n",
    "71. Time series / date functionality ... [Link](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases)\n",
    "72. Pyspark Column Transformation: Calculate Percentage Change for Each Group in a Column ... [Link](https://stackoverflow.com/questions/57470314/pyspark-column-transformation-calculate-percentage-change-for-each-group-in-a-c)\n",
    "73. Rate of Change (ROC) ... [Link](https://www.investopedia.com/terms/r/rateofchange.asp#:~:text=Rate%20of%20change%20is%20used,value%20from%20an%20earlier%20period.)\n",
    "74. 8 Examples of Data Lake Architectures on Amazon S3 ... [Link](https://www.upsolver.com/blog/examples-of-data-lake-architecture-on-amazon-s3)\n",
    "75. Immigration Dataset Dictionary ... [Link](https://knowledge.udacity.com/questions/529723)\n",
    "76. Data dictionary given in workspace, do we need to create reference table for some field ... [Link](https://knowledge.udacity.com/questions/230175)\n",
    "77. What is Data Dictionary ... [Link](https://www.tutorialspoint.com/What-is-Data-Dictionary)\n",
    "78. data dictionary ... [Link](https://knowledge.udacity.com/questions/196779)\n",
    "79. error output for long running process for Data Lake project ... [Link](https://knowledge.udacity.com/questions/131502)\n",
    "80. udacity/workspaces-student-support ... [Link](https://github.com/udacity/workspaces-student-support/tree/master/jupyter)\n",
    "81. Display the Pandas DataFrame in table style ... [Link](https://www.geeksforgeeks.org/display-the-pandas-dataframe-in-table-style/)\n",
    "82. Climate Change: Earth Surface Temperature Data ... [Link](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data?select=GlobalLandTemperaturesByCity.csv)\n",
    "83. Country code and names ... [Link](https://www.kaggle.com/leogenzano/country-code-and-names)\n",
    "84. U.S. Pollution Data ... [Link](https://www.kaggle.com/sogun3/uspollution)\n",
    "85. Animation, Basemap, Plotly for Air Quality Index ... [Link](https://www.kaggle.com/jaeyoonpark/animation-basemap-plotly-for-air-quality-index)\n",
    "86. Air Data Basic Information ... [Link](https://www.epa.gov/outdoor-air-quality-data/air-data-basic-information)\n",
    "87. Air Quality Index (AQI) ... [Link](https://www.epa.gov/sites/default/files/2014-05/documents/zell-aqi.pdf)\n",
    "88. U.S. Electric Power Generators ... [Link](https://www.kaggle.com/saurabhshahane/us-electric-power-generators)\n",
    "89. Form EIA-860 detailed data with previous form data (EIA-860A/860B) ... [Link](https://www.eia.gov/electricity/data/eia860/)\n",
    "90. Airline Database ... [Link](https://www.kaggle.com/open-flights/airline-database)\n",
    "91. Airport, airline and route data ... [Link](https://openflights.org/data.html#airline)\n",
    "92. Aviation-Data-Download ... [Link](https://www.aviationfile.com/aviation-data-download/)\n",
    "93. ICAO airport code ... [Link](https://en.wikipedia.org/wiki/ICAO_airport_code)\n",
    "94. Airline codes ... [Link](https://en.wikipedia.org/wiki/Airline_codes#:~:text=The%20ICAO%20airline%20designator%20is,designator%20and%20a%20telephony%20designator.)\n",
    "95. List of airline codes ... [Link](https://en.wikipedia.org/wiki/List_of_airline_codes)\n",
    "96. ISO 3166-1 alpha-2 ... [Link](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2)\n",
    "97. List of ISO 3166 country codes ... [Link](https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes#UNI4)\n",
    "98. Spark SAS Data Source (sas7bdat) ... [Link](https://github.com/saurfang/spark-sas7bdat)\n",
    "99. US Cities: Demographics ... [Link](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/?dataChart=eyJxdWVyaWVzIjpbeyJjb25maWciOnsiZGF0YXNldCI6InVzLWNpdGllcy1kZW1vZ3JhcGhpY3MiLCJvcHRpb25zIjp7fX0sImNoYXJ0cyI6W3siYWxpZ25Nb250aCI6dHJ1ZSwidHlwZSI6ImNvbHVtbiIsImZ1bmMiOiJBVkciLCJ5QXhpcyI6Im1lZGlhbl9hZ2UiLCJzY2llbnRpZmljRGlzcGxheSI6dHJ1ZSwiY29sb3IiOiIjRkY1MTVBIn1dLCJ4QXhpcyI6ImNpdHkiLCJtYXhwb2ludHMiOjUwLCJzb3J0IjoiIn1dLCJ0aW1lc2NhbGUiOiIiLCJkaXNwbGF5TGVnZW5kIjp0cnVlLCJhbGlnbk1vbnRoIjp0cnVlfQ%3D%3D)\n",
    "100. Scale up vs Scale out: What’s the difference? ... [Link](https://opsani.com/blog/scale-up-vs-scale-out-whats-the-difference/#:~:text=Scaling%20out%20is%20adding%20more,to%20handle%20a%20greater%20load.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
